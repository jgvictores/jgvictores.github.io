@inproceedings{montesino2022entorno,
   author = {Ignacio Montesino Valle and Bartek Łukawski and Juan G. Victores and Alberto Jardón Huete and Carlos Balaguer},
   doi = {10.17979/spudc.9788497498418.0762},
   journal = {XLIII Jornadas de Automática: libro de actas: 7, 8 y 9 de septiembre de 2022, Logroño (La Rioja)},
   month = {9},
   pages = {762-769},
   publisher = {Servizo de Publicacións da UDC},
   title = {Entorno de Gym basado en impedancia para el robot colaborativo IIWA de cara a interacción humano robot},
   url = {https://doi.org/10.17979/spudc.9788497498418.0762},
   year = {2022},
}
@inproceedings{lukawski2022aninverse,
   abstract = {Several methodologies exist for solving the inverse kinematics of a manipulator arm. Basing on screw theory, it is possible to efficiently obtain complete and exact solutions. An open-source C++ implementation of an automated problem solver of this kind is introduced, and a comparative with selected known algorithms is established using the TEO humanoid robot platform by Universidad Carlos III de Madrid. The Orocos Kinematics and Dynamics Library is used for geometry and motion-related operations},
   author = {Bartek Łukawski and Ignacio Montesino Valle and Juan G. Victores and Alberto Jardón and Carlos Balaguer},
   doi = {10.17979/spudc.9788497498418.0864},
   journal = {XLIII Jornadas de Automática: libro de actas: 7, 8 y 9 de septiembre de 2022, Logroño (La Rioja)},
   month = {9},
   pages = {864-869},
   publisher = {Servizo de Publicacións da UDC},
   title = {An inverse kinematics problem solver based on screw theory for manipulator arms},
   url = {https://doi.org/10.17979/spudc.9788497498418.0864},
   year = {2022},
}
@inproceedings{lukawski2023ageneric,
   abstract = {A usual form of human-robot interaction is the ability of the former to remotely command the latter through any sort of auxiliary device; this interaction is referred to with the term “teleoperation”. Robots are common examples of systems that can be controlled remotely. Depending on the task at hand, said systems can grow in complexity and costs. Specifically, the peripherals devoted to controlling the robot could require costly engineering and even an ad hoc design. However, a range of low-cost, commercial devices and controllers, originally intended for other purposes, can also be a good fit for teleoperation tasks in robotics. This work explores a selected collection of popular devices of this kind, and proposes a unified framework to exploit their capabilities as remote controllers for a set of robotic platforms. Their suitability is proven both on real and simulated versions of these platforms through simple experiments that show how they could be further used in more complex scenarios.},
   author = {Bartek Łukawski and Juan G. Victores and Carlos Balaguer},
   doi = {10.17979/spudc.9788497498609.785},
   journal = {XLIV Jornadas de Automática: libro de actas: Universidad de Zaragoza, Escuela de Ingeniería y Arquitectura, 6, 7 y 8 de septiembre de 2023, Zaragoza},
   month = {9},
   pages = {785-788},
   publisher = {Servizo de Publicacións. Universidade da Coruña},
   title = {A generic controller for teleoperation on robotic manipulators using low-cost devices},
   year = {2023},
}
@inproceedings{hernandez-perez2023ros2,
   abstract = {La rehabilitacion es una herramienta esencial que ayuda a las personas a restaurar la movilidad en las extremidades afectadas por diversas afecciones, como enfermedades neurologicas. Las terapias convencionales, que incluyen terapia ocupacional, física y del habla, se han mejorado con nuevas tecnologías, como  sistemas roboticos asistidos y juegos de realidad virtual y aumentada, para aumentar la participacion y, en consecuencia, la efectividad. Esta investigación se centra en la implementación de un dispositivo portatil de sensores de electromiograma (EMG) de ocho canales, el brazalete Mindrove, para el reconocimiento de gestos. El objetivo es desarrollar un modelo clasifcador utilizando el algoritmo de Maquinas de Vectores de Soporte (SVM) para distinguir ocho gestos diferentes de la mano y aplicarlo en un sistema de reconocimiento de gestos. El estudio demuestra la viabilidad de este sistema de reconocimiento y explora la aplicacion potencial de esta tecnología en juegos interactivos de Unity para terapia de rehabilitacion. Los resultados muestran una precisión prometedora en la clasifcación del modelo y se necesita más investigación para abordar los desaf´ıos relacionados con la especificidad del usuario y la precision del reconocimiento de estos. El trabajo futuro implica ampliar el repertorio de gestos reconocidos, incorporar datos adicionales del sensor y explorar tecnicas de extracción de características mas avanzadas para mejorar el rendimiento general del sistema de reconocimiento de gestos en terapias de rehabilitacion.},
   author = {Sofia Hernández Pérez and Ignacio Montesino Valle and Juan G. Victores and , Edwin Daniel Oña and Alberto Jardón Huete},
   doi = {10.17979/spudc.9788497498609.611},
   journal = {XLIV Jornadas de Automática: libro de actas: Universidad de Zaragoza, Escuela de Ingeniería y Arquitectura, 6, 7 y 8 de septiembre de 2023, Zaragoza},
   month = {9},
   pages = {611-616},
   publisher = {Servizo de Publicacións. Universidade da Coruña},
   title = {ROS2 gesture classification pipeline towards gamified neuro-rehabilitation therapy},
   url = {https://doi.org/10.17979/spudc.9788497498609.611},
   year = {2023},
}
@inproceedings{dematias2023planificador,
   abstract = {La navegacion autónoma de robots móviles es una disciplina muy estudiada hoy en día. Sin embargo, existen pocas aplicaciones que permitan planificar trayectorias seguras para un robot movil manipulador que se encuentra con el brazo en extensión, sobrepasando los límites de la base, ya que en la mayoría de los planificadores se supone la huella del robot como circular estableciendo su radio. Por ello, en este artículo se presenta un planificador SE(2) para robots moviles manipuladores programado en el entorno ROS, disponible para su uso mediante un plugin. Este tiene en cuenta el angulo de orientación del robot para sortear con mayor precaucion los obstáculos que se encuentren frente a él. Para ello, se crea un mapa 3D, cuyas dimensiones corresponden a los ejes X, Y y la orientacion, y se planifica sobre este la trayectoria. Este planificador ha sido comprobado en la simulación del robot TIAGo, comprobando que se evitan al navegar las colisiones del brazo de forma efectiva.},
   author = {Ainhoa De Matías-Martínez and Francisco J. Naranjo-Campos and Juan G Victores and Carlos Balaguer},
   city = {Madrid},
   isbn = {978-84-09-51892-0},
   journal = {Jornadas de Robótica y Bioingeniería},
   month = {6},
   pages = {85-90},
   publisher = {CEA UPM CSIC},
   title = {Planificador global SE(2) para la navegación de robots móviles manipuladores en ROS},
   year = {2023},
}
@incollection{victores2023tunnel,
   author = {Juan G. Victores and Elisabeth Menendez and Carlos Balaguer},
   doi = {10.1002/9781394162871.ch9},
   journal = {Infrastructure Robotics: Methodologies, Robotic Systems and Applications},
   pages = {185-203},
   publisher = {Wiley Online Library},
   title = {Tunnel Structural Inspection and Assessment Using an Autonomous Robotic System},
   url = {https://doi.org/10.1002/9781394162871.ch9},
   year = {2024},
}
@inproceedings{gago2020under-actuation,
   abstract = {This paper presents a study on under-actuation modelling applied to robotic hands aimed at sign language representation. Prior studies using a simulated TEO humanoid robot for representing sign language have shown positive comprehension and satisfaction responses among the deaf and hearing impaired community. The under-actuated mechanics of the robotic fingers were not contemplated in the simulated model, thus the correspondence problem arises as the previous joint space positions cannot be directly sent to the physical system. In addition to the 3:1 and 2:1 ratio of the under-actuation of the finger mechanisms, tendons and springs involve stiffness and elasticity that are difficult or unfeasible to model, and justify the need for a data-driven approach. Three motor command generators using three different neural network models are analysed and evaluated. Two of the generators are trained in a supervised fashion, and the third involves variational self-supervision and a transformation upon the latent space. The simulated joint space positions are translated into motor commands for the physical embodied robot to represent a sign language dactylology, which is in turn evaluated by deaf and hearing impaired end-users.},
   author = {Jennifer J. Gago and Bartek Łukawski and Juan G. Victores and Carlos Balaguer},
   city = {Cham},
   editor = {Paulo
and Camacho David
and Yin Hujun Analide Cesar
and Novais},
   isbn = {978-3-030-62365-4},
   journal = {Intelligent Data Engineering and Automated Learning – IDEAL 2020},
   pages = {239-251},
   publisher = {Springer International Publishing},
   title = {Under-Actuation Modelling in Robotic Hands via Neural Networks for Sign Language Representation with End-User Validation},
   url = {https://doi.org/10.1007/978-3-030-62365-4_23},
   year = {2020},
}
@article{delatorre2024spasticsim,
   abstract = {In neurorehabilitation, assessment of functional problems is essential to define optimal rehabilitation treatments. Usually, this assessment process requires distinguishing between impaired and non-impaired behavior of limbs. One of the common muscle motor disorders affecting limbs is spasticity, which is complicated to quantify objectively due to the complex nature of motor control. Thus, the lack of heterogeneous samples of patients constituting an acceptable amount of data is an obstacle which is relevant to understanding the behavior of spasticity and, consequently, quantifying it. In this article, we use the 3D creation suite Blender combined with the MBLab add-on to generate synthetic samples of human body models, aiming to be as sufficiently representative as possible to real human samples. Exporting these samples to OpenSim and performing four specific upper limb movements, we analyze the muscle behavior by simulating the six degrees of spasticity contemplated by the Modified Ashworth Scale (MAS). The complete dataset of patients and movements is open-source and available for future research. This approach advocates the potential to generate synthetic data for testing and validating musculoskeletal models.},
   author = {Rubén de-la-Torre and Edwin Daniel Oña and Juan G. Victores and Alberto Jardón},
   doi = {10.1038/s41598-024-51993-w},
   issn = {2045-2322},
   issue = {1},
   journal = {Scientific Reports},
   month = {1},
   pages = {1646},
   title = {SpasticSim: a synthetic data generation method for upper limb spasticity modelling in neurorehabilitation},
   volume = {14},
   url = {https://doi.org/10.1038/s41598-024-51993-w},
   year = {2024},
}
@article{fernandezfernandez2023deep,
   abstract = {The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application. The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks. Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot. Results are compared in terms of similarity and obtained reward with respect to the reference inputs.},
   author = {Raul Fernandez-Fernandez and Juan G. Victores and Carlos Balaguer},
   doi = {10.1016/J.COGSYS.2023.05.004},
   issn = {1389-0417},
   journal = {Cognitive Systems Research},
   keywords = {Deep Q-Networks,Deep Reinforcement Learning,Robotic art,Robotics},
   month = {9},
   pages = {57-63},
   publisher = {Elsevier},
   title = {Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching},
   volume = {81},
   url = {https://doi.org/10.1016/J.COGSYS.2023.05.004},
   year = {2023},
}
@article{fernandezfernandez2023transferring,
   abstract = {Neural Style Transfer (NST) was originally proposed to use feature extraction capabilities of Neural Networks as a way to perform Style Transfer with images. Pre-trained image classification architectures were selected for feature extraction, leading to new images showing the same content as the original but with a different style. In robotics, Style Transfer can be employed to transfer human motion styles to robot motions. The challenge lies in the lack of pre-trained classification architectures for robot motions that could be used for feature extraction. Neural Policy Style Transfer TD3 (NPST3) is proposed for the transfer of human motion styles to robot motions. This framework allows the same robot motion to be executed in different human-centered motion styles, such as in an “angry”, “happy”, “calm”, or “sad” fashion. The Twin Delayed Deep Deterministic Policy Gradient (TD3) network is introduced for the generation of control policies. An autoencoder network is in charge of feature extraction for the Style Transfer step. The Style Transfer step can be performed both offline and online: offline for the autonomous executions of human-style robot motions, and online for adapting at runtime the style of e.g., a teleoperated robot. The framework is tested using two different robotic platforms: a robotic manipulator designed for telemanipulation tasks, and a humanoid robot designed for social interaction. The proposed approach was evaluated for both platforms, performing a total of 147 questionnaires asking human subjects to recognize the human motion style transferred to the robot motion for a predefined set of actions.},
   author = {Raul Fernandez-Fernandez and Bartek Łukawski and Juan G. Victores and Claudio Pacchierotti},
   doi = {10.1016/J.COGSYS.2023.05.010},
   issn = {1389-0417},
   journal = {Cognitive Systems Research},
   keywords = {Autoencoders,Deep reinforcement learning,NPST,Style transfer,TD3},
   month = {12},
   pages = {101121},
   publisher = {Elsevier},
   title = {Transferring human emotions to robot motions using Neural Policy Style Transfer},
   volume = {82},
   url = {https://doi.org/10.1016/J.COGSYS.2023.05.010},
   year = {2023},
}
@inproceedings{fernandezfernandez2022neuralB,
   abstract = {Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the “what” and the Style as the “how” of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an “angry”, “happy”, “calm”, or “sad” way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST33) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments, or online, for adapting at runtime the style of a teleoperated robot. The considered styles can be learned online from human demonstrations. We carried out an evaluation with human subjects enrolling 73 volunteers, asking them to recognize the style behind some representative robotic motions. Results show a good recognition rate, proving that it is possible to convey different styles to a robot using this approach.},
   author = {Raul Fernandez-Fernandez and Marco Aggravi and Paolo Robuffo Giordano and Juan G Victores and Claudio Pacchierotti},
   doi = {10.1109/ICRA46639.2022.9812245},
   journal = {2022 International Conference on Robotics and Automation (ICRA)},
   month = {5},
   pages = {4073-4079},
   title = {Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators},
   url = {https://doi.org/10.1109/ICRA46639.2022.9812245},
   year = {2022},
}
@article{fernandezfernandez2022neural,
   abstract = {Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST)11NPST: Neural Policy Style Transfer. algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content policy and a style policy to the NPST algorithm. Experiments are performed in a catch-ball game inspired by the Deep Reinforcement Learning classical Atari games; and a real-world painting scenario with a full-sized humanoid robot, based on previous works of the authors. The implementation of three different Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode the policies within the NPST framework is proposed and the results obtained in the experiments with each of these architectures compared.},
   author = {Raul Fernandez-Fernandez and Juan G Victores and Jennifer J Gago and David Estevez and Carlos Balaguer},
   doi = {10.1016/j.cogsys.2021.11.003},
   issn = {1389-0417},
   journal = {Cognitive Systems Research},
   keywords = {Deep learning,Deep reinforcement learning,Robotics,Style Transfer},
   month = {3},
   pages = {23-32},
   title = {Neural Policy Style Transfer},
   volume = {72},
   url = {https://doi.org/10.1016/j.cogsys.2021.11.003},
   year = {2022},
}
@article{gil2020design,
   abstract = {The inspection of Personal Protective Equipment (PPE) is one of the most necessary measures when treating patients affected by infectious diseases, such as Ebola or COVID-19. Assuring the integrity of health personnel in contact with infected patients has become an important concern in developed countries. This work focuses on the study of Reinforcement Learning (RL) techniques for controlling a scanner prototype in the presence of blood traces on the PPE that could arise after contact with pathological patients. A preliminary study on the design of an agent-environment system able to simulate the required task is presented. The task has been adapted to an environment for the OpenAI Gym toolkit. The evaluation of the agent&rsquo;s performance has considered the effects of different topological designs and tuning hyperparameters of the Q-Learning model-free algorithm. Results have been evaluated on the basis of average reward and timesteps per episode. The sample-average method applied to the learning rate parameter, as well as a specific epsilon decaying method worked best for the trained agents. The obtained results report promising outcomes of an inspection system able to center and magnify contaminants in the real scanner system.},
   author = {Andrea Gil Ruiz and Juan G Victores and Bartek Łukawski and Carlos Balaguer},
   doi = {10.3390/app10175927},
   issn = {2076-3417},
   issue = {17},
   journal = {Applied Sciences},
   month = {8},
   pages = {5927},
   title = {Design of an Active Vision System for High-Level Isolation Units through Q-Learning},
   volume = {10},
   url = {https://doi.org/10.3390/app10175927},
   year = {2020},
}
@article{gago2019sign,
   abstract = {In this paper, we illustrate our work on improving the accessibility of Cyber-Physical Systems (CPS), presenting a study on human-robot interaction where the end-users are either deaf or hearing-impaired people. Current trends in robotic designs include devices with robotic arms and hands capable of performing manipulation and grasping tasks. This paper focuses on how these devices can be used for a different purpose, which is that of enabling robotic communication via sign language. For the study, several tests and questionnaires are run to check and measure how end-users feel about interpreting sign language represented by a humanoid robotic assistant as opposed to subtitles on a screen. Stemming from this dichotomy, dactylology, basic vocabulary representation and end-user satisfaction are the main topics covered by a delivered form, in which additional commentaries are valued and taken into consideration for further decision taking regarding robot-human interaction. The experiments were performed using TEO, a household companion humanoid robot developed at the University Carlos III de Madrid (UC3M), via representations in Spanish Sign Language (LSE), and a total of 16 deaf and hearing-impaired participants.62D05, 62Q05, 62K99,},
   author = {Jennifer J. Gago and Juan G. Victores and Carlos Balaguer},
   doi = {10.3390/electronics8010057},
   issn = {2079-9292},
   issue = {1},
   journal = {Electronics},
   keywords = {Accessibility,Anthropomorphic robotic hands,Assistive robotics,Cyber,Cyber-Physical Systems,Dactylology,Household companion,Human-robot interaction,Humanoid,Physical Systems,Robotics,Sign language,Statistics,Survey,Vocabulary,accessibility,anthropomorphic robotic hands,assistive robotics,dactylology,household companion,human,humanoid,robot interaction,robotics,sign language,statistics,survey,vocabulary},
   month = {1},
   pages = {57},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Sign language representation by TEO humanoid robot: End-user interest, comprehension and satisfaction},
   volume = {8},
   url = {https://doi.org/10.3390/electronics8010057},
   year = {2019},
}
@inproceedings{sierragarcia2019neural,
   abstract = {Floating offshore wind turbines (FOWT) are exposed to hard environmental conditions which could impose expensive maintenance operations. These costs could be alleviated by monitoring these floating devices using UAVs. Given the FOWT location, UAVs are currently the only way to do this health monitoring. But this means that UAV should be well equipped and must be accurately controlled. Rotational inertia variation is a common disturbance that affect the aerial vehicles during these inspection tasks. To address this issue, in this work we propose a new neural controller based on adaptive neuro estimators. The approach is based on the hybridization of feedback linearization, PIDs and artificial neural networks. Online learning is used to help the network to improve the estimations while the system is working. The proposal is tested by simulation with several complex trajectories when the rotational inertia is multiplied by 10. Results show the proposed UAV neural controller gets a good tracking and the neuro estimators tackle the effect of the variations of the rotational inertia.},
   author = {J. Enrique Sierra-Garcia and Matilde Santos and Juan G. Victores},
   city = {Manchester},
   doi = {10.1007/978-3-030-33617-2_19},
   isbn = {978-3-030-33617-2},
   journal = {IDEAL 2019},
   keywords = {FOWT,Inertia variations,UAV,neural networks,neuro-estimator},
   month = {11},
   pages = {169-177},
   publisher = {Springer International Publishing},
   title = {Neural controller of UAVs with inertia variations},
   url = {https://doi.org/10.1007/978-3-030-33617-2_19},
   year = {2019},
}
@article{estevez2019enabling,
   abstract = {Domestic chores, such as laundry tasks, are dull and repetitive. These tasks consume a significant amount of daily time, and are however unavoidable. Additionally, a great portion of elder and disabled people require help to perform them due to lack of mobility. In this work we present advances towards a Robot Household Companion (RHC), focusing on the performance of two particular laundry tasks: unfolding and ironing garments. Unfolding is required to recognize the garment prior to any later folding operation. For unfolding, we apply an interactive algorithm based on the analysis of a colored 3D reconstruction of the garment. Regions are clustered based on height, and a bumpiness value is computed to determine the most suitable pick and place points to unfold the overlapping region. For ironing, a custom Wrinkleness Local Descriptor (WiLD) descriptor is applied to a 3D reconstruction to find the most significant wrinkles in the garment. These wrinkles are then ironed using an iterative path-following control algorithm that regulates the amount of pressure exerted on the garment. Both algorithms focus on the feasibility of a physical implementation in real unmodified environments. A set of experiments to validate the algorithms have been performed using a full-sized humanoid robot.},
   author = {David Estevez and Juan G. Victores and Raul Fernandez-Fernandez and Carlos Balaguer},
   doi = {10.1016/j.robot.2019.103330},
   issn = {0921-8890},
   journal = {Robotics and Autonomous Systems},
   keywords = {Computer vision,Deformable objects,Force/torque control,Garments,Ironing,Robotics},
   month = {1},
   pages = {103330},
   publisher = {Elsevier},
   title = {Enabling Garment-Agnostic Laundry Tasks For A Robot Household Companion},
   volume = {123},
   url = {https://doi.org/10.1016/j.robot.2019.103330},
   year = {2020},
}
@inproceedings{gago2019sequence,
   abstract = {This paper presents a study on natural language to sign language translation with human-robot interaction application purposes. By means of the presented methodology, the humanoid robot TEO is expected to represent Spanish sign language automatically by converting text into movements, thanks to the performance of neural networks. Natural language to sign language translation presents several challenges to developers, such as the discordance between the length of input and output data and the use of non-manual markers. Therefore, neural networks and, consequently, sequence-to-sequence models, are selected as a data-driven system to avoid traditional expert system approaches or temporal dependencies limitations that lead to limited or too complex translation systems. To achieve these objectives, it is necessary to find a way to perform human skeleton acquisition in order to collect the signing input data. OpenPose and skeletonRetriever are proposed for this purpose and a 3D sensor specification study is developed to select the best acquisition hardware.},
   author = {Jennifer J. Gago and Valentina Vasco and Bartek Łukawski and Ugo Pattacini and Vadim Tikhanoff and Juan G. Victores and Carlos Balaguer},
   city = {Logroño},
   doi = {10.11128/arep.58},
   isbn = {978-3-901608-92-6},
   journal = {EUROSIM 2019},
   keywords = {Humanoid Robot,Sequence-to-Sequence,Sign Language,Translator},
   month = {7},
   pages = {44},
   publisher = {ARGESIM},
   title = {Sequence-to-Sequence Natural Language to Humanoid Robot Sign Language},
   url = {https://www.doi.org/10.11128/arep.58},
   year = {2019},
}
@inproceedings{estevez2019towards,
   abstract = {People spend several hours a week doing laundry, with hanging clothes being one of the laundry tasks to be performed. Nevertheless, deformable object manipulation still proves to be a challenge for most robotic systems, due to the extremely large number of internal degrees of freedom of a piece of clothing and its chaotic nature. This work presents a step towards automated robot clothes hanging by modelling the dynamics of the hanging task via deep convolutional models. Two models are developed to address two different problems: determining if the garment will hang or not (classification), and estimating the future garment location in space (regression). Both models have been trained with a synthetic dataset formed by 15 000 examples generated though a dynamic simulation of a deformable object. Experiments show that the deep convolutional models presented perform better than a human expert, and that future predictions are largely influenced by time, with uncertainty influencing directly the accuracy of the predictions.},
   author = {David Estevez and Juan G. Victores and Raul Fernandez-Fernandez and Carlos Balaguer},
   city = {Logroño},
   doi = {10.11128/arep.58},
   isbn = {978-3-901608-92-6},
   journal = {EUROSIM 2019},
   keywords = {deep learning,deformable objects,laundry,robotics},
   month = {7},
   pages = {35},
   publisher = {ARGESIM},
   title = {Towards Clothes Hanging via Cloth Simulation and Deep Convolutional Networks},
   url = {https://www.doi.org/10.11128/arep.58},
   year = {2019},
}
@inproceedings{fernandezfernandez2019quick,
   abstract = {The Quick, Draw! Dataset is a Google dataset with a collection of 50 million drawings, divided in 345 categories, collected from the users of the game Quick, Draw!. In contrast with most of the existing image datasets, in the Quick, Draw! Dataset, drawings are stored as time series of pencil positions instead of a bitmap matrix composed by pixels. This aspect makes this dataset the largest doodle dataset available at the time. The Quick, Draw! Dataset is presented as a great opportunity to researchers for developing and studying machine learning techniques. Due to the size of this dataset and the nature of its source, there is a scarce of information about the quality of the drawings contained. In this paper a statistical analysis of three of the classes contained in the Quick, Draw! Dataset is depicted: mountain, book and whale. The goal is to give to the reader a first impression of the data collected in this dataset. For the analysis of the quality of the drawings a Classification Neural Network was trained to obtain a classification score. Using this classification score and the parameters provided by the dataset, a statistical analysis of the quality and nature of the drawings contained in this dataset is provided.},
   author = {Raul Fernandez-Fernandez and Juan G. Victores and David Estevez and Carlos Balaguer},
   city = {Logroño},
   doi = {10.11128/arep.58},
   isbn = {978-3-901608-92-6},
   journal = {EUROSIM 2019},
   keywords = {Neural Networks,Quick Draw! Dataset,Statistical Analysis},
   month = {7},
   pages = {27},
   publisher = {ARGESIM},
   title = {Quick, Stat!: A Statistical Analysis of the Quick, Draw! Dataset},
   url = {https://www.doi.org/10.11128/arep.58},
   year = {2019},
}
@article{stazio2019astudy,
   abstract = {The examination of Personal Protective Equipment (PPE) to assure the complete integrity of health personnel in contact with infected patients is one of the most necessary tasks when treating patients affected by infectious diseases, such as Ebola. This work focuses on the study of machine vision techniques for the detection of possible defects on the PPE that could arise after contact with the aforementioned pathological patients. A preliminary study on the use of image classification algorithms to identify blood stains on PPE subsequent to the treatment of the infected patient is presented. To produce training data for these algorithms, a synthetic dataset was generated from a simulated model of a PPE suit with blood stains. Furthermore, the study proceeded with the utilization of images of the PPE with a physical emulation of blood stains, taken by a real prototype. The dataset reveals a great imbalance between positive and negative samples; therefore, all the selected classification algorithms are able to manage this kind of data. Classifiers range from Logistic Regression and Support Vector Machines, to bagging and boosting techniques such as Random Forest, Adaptive Boosting, Gradient Boosting and eXtreme Gradient Boosting. All these algorithms were evaluated on accuracy, precision, recall and F1 score; and additionally, execution times were considered. The obtained results report promising outcomes of all the classifiers, and, in particular Logistic Regression resulted to be the most suitable classification algorithm in terms of F1 score and execution time, considering both datasets.},
   author = {Alice Stazio and Juan G. Victores and David Estevez and Carlos Balaguer},
   doi = {10.3390/electronics8070743},
   issue = {7},
   journal = {Electronics},
   keywords = {AdaBoost,Personal Protective Equipment (PPE),Support Vector Machine (SVM),class imbalance,healthcare,infectious diseases,machine vision,physical emulation,synthetic dataset},
   month = {6},
   pages = {743},
   title = {A Study on Machine Vision Techniques for the Inspection of Health Personnels’ Protective Suits for the Treatment of Patients in Extreme Isolation},
   volume = {8},
   url = {https://doi.org/10.3390/electronics8070743},
   year = {2019},
}
@inproceedings{gago2019astudy,
   abstract = {The humanoid robot TEO is an assistive household companion which is able to represent Spanish Sign Language (LSE) vocabulary and dactylology thanks to its underactuated robotic hands. Prior studies on sign language representation through the simulation of the humanoid robot TEO revealed that end-users’ predisposition to communicate with robots via sign language was over 80% positive. Moreover, around 65% of reticent end-users changed their minds after their first experience with the simulated robot. Unexpectedly, a new developed study has shown that embodiment has dropped satisfaction rates drastically and increased comprehension rates. The under-actuated movement of the real hands has been modelled according to three generators based on three different neural networks, and the data obtained in previous simulation. Therefore, four different dactylology models have been shown to end-users. It has to be considered that the notion of embodiment is required where there is cohesive interaction between the environment and the body. Despite this, recent end-user feedback has shown some recurring criticisms referring to the embodied robot that did not arise with the simulation experiments. Among the most recurrent topics, the demand of facial expression is emphasised. In terms of demographics, the decision of grouping the academic background groups into two main sectors (university and non-university studies) relies on the link between university and research. Therefore, university students and graduates are expected to be more aware of actual robotics state of the art and, consequently, to be more critical towards the scientific implementation. As expected, lower satisfaction rates are detected among university students and graduates. Differences in comprehension rates are not significant. Nevertheless, a more pronounced overall age-related comprehension decreasing trend is identified for non-university graduates.},
   author = {Jennifer J. Gago and Bartek Łukawski and Juan G. Victores and Carlos Balaguer},
   city = {Prague},
   journal = {Gesture-Sign Workshop},
   pages = {26-28},
   title = {A Study on the Effects of an Embodied Humanoid Robot Representing Sign Language},
   url = {https://calc.ff.cuni.cz/en/gswp19_programme/},
   year = {2019},
}
@inproceedings{estevez2017improving,
   abstract = {Current approaches for robotic garment folding require a full view of an extended garment, in order to successfully apply a model-based folding sequence. In this paper, we present a garment-agnostic algorithm that requires no model to unfold clothes and works using only depth data. Once the garment is unfolded, state of the art approaches for folding may be applied. The algorithm presented is divided into 3 main stages. First, a Segmentation stage extracts the garment data from the background, and approximates its contour into a polygon. Then, a Clustering stage groups regions of similar height within the garment, corresponding to different overlapped regions. Finally, a Pick and Place Points stage finds the most suitable points for grasping and releasing the garment for the unfolding process, based on a bumpiness value defined as the accumulated difference in height along selected candidate paths. Experiments for evaluation of the vision algorithm have been performed over a dataset of 30 samples from a total of 6 different garment categories with one and two folds. The whole unfolding algorithm has also been validated through experiments with an industrial robot platform over a subset of the dataset garments.},
   author = {David Estevez and Raul Fernandez-Fernandez and Juan G. Victores and Carlos Balaguer},
   doi = {10.1109/ICARSC.2017.7964077},
   isbn = {9781509062331},
   journal = {2017 IEEE International Conference on Autonomous Robot Systems and Competitions, ICARSC 2017},
   publisher = {IEEE},
   title = {Improving and evaluating robotic garment unfolding: A garment-agnostic approach},
   url = {https://doi.org/10.1109/ICARSC.2017.7964077},
   year = {2017},
}
@inproceedings{fernandez-fernandez2017improving,
   abstract = {In the Continuous Goal Directed Actions (CGDA) framework, actions are modelled as time series which contain the variations of object and environment features. As robot joint trajectories are not explicitly encoded in CGDA, Evolutionary Algorithms (EA) are used for the execution of these actions. These computations usually require a large number of evaluations. As a consequence of this, these evaluations are performed in a simulated environment, and the computed trajectory is then transferred to the physical robot. In this paper, constraints are introduced in the CGDA framework, as a way to reduce the number of evaluations needed by the system to converge to the optimal robot joint trajectory. Specifically, spatial and velocity constraints are introduced in the framework. Their effects in two different CGDA commonly studied use cases (the 'wax' and 'paint' actions) are analyzed and compared. The experimental results obtained using these constraints are compared with those obtained with the Steady State Tournament (SST) algorithm used in the original proposal of CGDA. Conclusions extracted from this study depict a high reduction in the required number of evaluations when incorporating spatial constraints. Velocity constraints provide however less promising results, which will be discussed within the context of previous CGDA works.},
   author = {Raul Fernandez-Fernandez and David Estevez and Juan G. Victores and Carlos Balaguer},
   doi = {10.1109/ICARSC.2017.7964090},
   isbn = {9781509062331},
   journal = {2017 IEEE International Conference on Autonomous Robot Systems and Competitions, ICARSC 2017},
   pages = {290-295},
   publisher = {IEEE},
   title = {Improving CGDA execution through Genetic Algorithms incorporating Spatial and Velocity constraints},
   url = {https://doi.org/10.1109/ICARSC.2017.7964090},
   year = {2017},
}
@article{fernandezfernandez2018real,
   abstract = {One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users. Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through action demonstrations. In classical robot imitation frameworks, actions are modelled using joint or Cartesian space trajectories. They accurately describe actions where geometrical characteristics are relevant, such as fixed trajectories from one pose to another. Other features, such as visual ones, are not always well represented with these pure geometrical approaches. Continuous Goal-Directed Actions (CGDA) is an alternative to these conventional methods, as it encodes actions as changes of any selected feature that can be extracted from the environment. As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding. This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot. The current strategies involve performing evaluations in a simulated environment, transferring only the final joint trajectory to the actual robot. Smart City applications involve working in highly dynamic and complex environments, where having a precise model is not always achievable. Our goal is to study the tractability of performing these evaluations directly in a real-world scenario. Two different approaches to reduce the number of evaluations using EA, are proposed and compared. In the first approach, Particle Swarm Optimization (PSO)-based methods have been studied and compared within the CGDA framework: naïve PSO, Fitness Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO (AFFG-PSO). The second approach studied the introduction of geometrical and velocity constraints within the CGDA framework. The effects of both approaches were analyzed and compared in the “wax” and “paint” actions, two CGDA commonly studied use cases. Results from this paper depict an important reduction in the number of required evaluations.},
   author = {Raul Fernandez-Fernandez and Juan G. Victores and David Estevez and Carlos Balaguer},
   doi = {10.3390/s18113818},
   issn = {1424-8220},
   issue = {11},
   journal = {Sensors},
   keywords = {CGDA,LfD,PSO,PbD,Smart City,constraints,evaluations,evolutionary algorithms,fitness inheritance,humanoids robots,real world},
   month = {11},
   pages = {3818},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications},
   volume = {18},
   url = {https://doi.org/10.3390/s18113818},
   year = {2018},
}
@inproceedings{fernandezfernandez2018robot,
   abstract = {Continuous Goal-Directed Actions (CGDA) is a robot imitation framework that encodes actions as the changes they produce on the environment. While it presents numerous advantages with respect to other robot imitation frameworks in terms of generalization and portability, final robot joint trajectories for the execution of actions are not necessarily encoded within the model. This is studied as an optimization problem, and the solution is computed through evolutionary algorithms in simulated environments. Evolutionary algorithms require a large number of evaluations, which had made the use of these algorithms in real world applications very challenging. This paper presents online evolutionary strategies, as a change of paradigm within CGDA execution. Online evolutionary strategies shift and merge motor execution into the planning loop. A concrete online evolutionary strategy, Online Evolved Trajectories (OET), is presented. OET drastically reduces computational times between motor executions, and enables working in real world dynamic environments and/or with human collaboration. Its performance has been measured against Full Trajectory Evolution (FTE) and Incrementally Evolved Trajectories (IET), obtaining the best overall results. Experimental evaluations are performed on the TEO full-sized humanoid robot with “paint” and “iron” actions that together involve vision, kinesthetic and force features.},
   author = {Raul Fernandez-Fernandez and Juan G. Victores and David Estevez and Carlos Balaguer},
   city = {Madrid},
   doi = {IROS.2018.8593724},
   isbn = {978-1-5386-8094-0},
   issn = {2153-0866},
   journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
   pages = {6546-6551},
   publisher = {IEEE},
   title = {Robot Imitation through Vision, Kinesthetic and Force Features with Online Adaptation to Changing Environments},
   url = {https://doi.org/10.1109/IROS.2018.8593724},
   year = {2018},
}
@inproceedings{estevez2017roboticB,
   abstract = {As robotic systems become more popular in household environments, the complexity of required tasks also increases. In this work we focus on a domestic chore deemed dull by a majority of the population, the task of ironing. The presented algorithm improves on the limited number of previous works by joining 3D perception with force/torque sensing, with emphasis on finding a practical solution with a feasible implementation in a domestic setting. Our algorithm obtains a point cloud representation of the working environment. From this point cloud, the garment is segmented and a custom Wrinkleness Local Descriptor (WiLD) is computed to determine the location of the present wrinkles. Using this descriptor, the most suitable ironing path is computed and, based on it, the manipulation algorithm performs the force-controlled ironing operation. Experiments have been performed with a humanoid robot platform, proving that our algorithm is able to detect successfully wrinkles present in garments and iteratively reduce the wrinkleness using an unmodified iron.},
   author = {David Estevez and Juan G. Victores and Raul Fernandez-Fernandez and Carlos Balaguer},
   city = {Vancouver, BC, Canada},
   doi = {10.1109/IROS.2017.8206556},
   editor = {IEEE},
   isbn = {9781538626818},
   issn = {21530866},
   journal = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
   keywords = {3D perception,Clothing,Image color analysis,Iron,Robot sensing systems,Three-dimensional displays,custom Wrinkleness Local Descriptor,domestic appliances,domestic chore,feedback,force control,force feedback,force sensing,force-controlled ironing operation,garment,home automation,household environments,humanoid robot platform,humanoid robots,manipulation algorithm,point cloud representation,robotic systems,torque feedback,torque sensing},
   pages = {6484-6489},
   publisher = {IEEE},
   title = {Robotic ironing with 3D perception and force/torque feedback in household environments},
   volume = {2017-Septe},
   url = {https://doi.org/10.1109/IROS.2017.8206556},
   year = {2017},
}
@article{martinez2018experimental,
   abstract = {The computational complexity of humanoid robot balance control is reduced through the application of simplified kinematics and dynamics models. However, these simplifications lead to the introduction of errors that add to other inherent electro-mechanic inaccuracies and affect the robotic system. Linear control systems deal with these inaccuracies if they operate around a specific working point but are less precise if they do not. This work presents a model improvement based on the Linear Inverted Pendulum Model (LIPM) to be applied in a non-linear control system. The aim is to minimize the control error and reduce robot oscillations for multiple working points. The new model, named the Dynamic LIPM (DLIPM), is used to plan the robot behavior with respect to changes in the balance status denoted by the zero moment point (ZMP). Thanks to the use of information from force-torque sensors, an experimental procedure has been applied to characterize the inaccuracies and introduce them into the new model. The experiments consist of balance perturbations similar to those of push-recovery trials, in which step-shaped ZMP variations are produced. The results show that the responses of the robot with respect to balance perturbations are more precise and the mechanical oscillations are reduced without comprising robot dynamics.},
   author = {Santiago Martinez and Juan Miguel Garcia-Haro and Juan G. Victores and Alberto Jardon and Carlos Balaguer},
   doi = {10.3390/s18030836},
   issn = {14248220},
   issue = {3},
   journal = {Sensors},
   keywords = {Balance control,Force-torque sensors,Humanoid robot,Simplified model},
   month = {3},
   pages = {836},
   title = {Experimental robot model adjustments based on force-torque sensor information},
   volume = {18},
   url = {https://doi.org/10.3390/s18030836},
   year = {2018},
}
@article{menendez2018tunnel,
   abstract = {This paper presents the ROBO-SPECT European FP7 project, funded under the ICT-2013.2.2 programme on Robotics use cases & Accompanying measures, a robotized alternative to manual tunnel structural inspection and assessment of cracks and other defects. Physical developments include the design and implementation of a multi-degree-of-freedom robotic system, composed by a mobile vehicle, an extended crane, and a high precision robotic arm. A semi-supervised computer vision system to detect tunnel defects, and a ultrasonic sensor (US) robotic tool to measure width and depth of detected cracks have been also developed. An overview of defect detection methods, as well as the fundamental aspects of the project architecture and design, will be presented. In addition, the developed and implemented arm tip positioning algorithm for the US robotic tool shall be detailed. Finally, experimental evidence and details on validation in a real motorway tunnel with ongoing traffic will be provided.},
   author = {Elisabeth Menendez and Juan G. Victores and Roberto Montero and Santiago Martínez and Carlos Balaguer},
   doi = {10.1016/j.autcon.2017.12.001},
   issn = {09265805},
   journal = {Automation in Construction},
   keywords = {Automation,Computer vision,Control of robotic systems,Design,Inspection,Maintenance,Mechatronic systems,Tunnel},
   month = {3},
   pages = {117-126},
   title = {Tunnel structural inspection and assessment using an autonomous robotic system},
   volume = {87},
   url = {https://doi.org/10.1016/j.autcon.2017.12.001},
   year = {2018},
}
@article{loupos2017autonomous,
   abstract = {This paper presents a robotic platform, capable of autonomous tunnel inspection, developed under ROBO-SPECT European union funded research project. The robotic vehicle consists of a robotized production boom lift, a high precision robotic arm, advanced computer vision systems, a 3D laser scanner and an ultrasonic sensor. The autonomous inspection of tunnels requires advanced capabilities of the robotic vehicle and the computer vision sub-system. The robot localization in underground spaces and on long linear paths is a challenging task, as well as the mm accurate positioning of a robotic tip installed on a five-ton crane vehicle. Moreover, the 2D and 3D vision tasks, which support the inspection process, should tackle with poor and variable lighting conditions, low textured lining surfaces and the need for high accuracy. This contribution describes the final robotic vehicle and the developments as designed for concrete lining tunnel inspection. Results from the validation and benchmarking of the system are also included following the final tests at the operating Egnatia Motorway tunnels in northern Greece.},
   author = {Konstantinos Loupos and Anastasios D Doulamis and Christos Stentoumis and Eftychios Protopapadakis and Konstantinos Makantasis and Nikolaos D Doulamis and Angelos Amditis and Philippe Chrobocinski and Juan G. Victores and Roberto Montero and Elisabeth Menendez and Carlos Balaguer and Rafa Lopez and Miquel Cantero and Roman Navarro and Alberto Roncaglia and Luca Belsito and Stephanos Camarinopoulos and Nikolaos Komodakis and Praveer Singh},
   doi = {10.1007/s41315-017-0031-9},
   issn = {2366-598X},
   journal = {International Journal of Intelligent Robotics and Applications},
   pages = {1-24},
   title = {Autonomous robotic system for tunnel structural inspection and assessment},
   url = {https://doi.org/10.1007/s41315-017-0031-9},
   year = {2017},
}
@inproceedings{menendez2017sistema,
   abstract = {La inspección y mantenimiento de túneles son necesarias para que estas infraestructuras continúen funcionando correctamente. Estas operaciones son normalmente realizadas de forma manual por operadores expertos requiriendo mucho tiempo y sin garantías de un control de calidad. La inspección y el mantenimiento con sistemas automatizados proporcionan una mayor productividad, calidad y repetibilidad. Este trabajo presenta el proyecto europeo ROBO-SPECT dentro del séptimo programa marco (FP7). El principal objetivo de ROBO-SPECT es proporcionar una alternativa automatizada, más rápida y fiable a la inspección manual de túneles. El proyecto se centra en el diseño e implementación de un sistema robótico con múltiples grados de libertad, compuesto por un vehículo, una grúa extensible y un brazo robótico de gran precisión. Un sistema de visión artificial para detectar defectos en túneles y una herramienta con sensor de ultrasonidos para medir el ancho y la profundidad de las grietas detectadas también han sido desarrollados.},
   author = {Elisabeth Menendez and Juan G. Victores and Carlos Balaguer},
   journal = {Jornadas Nacionales de Robótica},
   keywords = {automatización,control autónomo,inspección,mantenimiento,robots,túnel},
   title = {Sistema robótico para la inspección y análisis estructural de túneles},
   url = {http://jnr2017.ai2.upv.es/wp-content/uploads/2016/11/Programa-JNR2017.pdf},
   year = {2017},
}
@inproceedings{estevez2017horus,
   abstract = {Las enfermedades altamente contagiosas son tratadas mediante procedimientos muy estrictos de la OMS. En el caso del Ébola, que tiene una tasa de letalidad que puede llegar al 90%, tanto enfermos como el personal sanitario deben seguir un protocolo mucho más estricto, el cual está bien definido aunque hay áreas de mejora importante con relación a la seguridad. Una de las tareas más delicadas es la retirada del traje protector EPI de los sanitarios en la antesala (esclusa) de la habitación del enfermo después de visitarlo. Puede ocurrir que el traje esté roto y/o contaminado con restos de sangre, vómitos, orina y, en general, con fluidos diversos. Precisamente por ello el objetivo de este proyecto es desarrollar tecnologías robóticas para asegurar la integridad del traje de protección del personal sanitario que trata el Ébola en tres ámbitos: 1) sensorización de la esclusa con cámaras de visión para la inspección automática de los trajes, 2) inspección robotizada más detallada del traje con sensores de detección de fluidos, y 3) creación de una base de datos dinámica de defectos y tipos de fluidos que aparecen en los trajes. Todo ello bajo dos premisas: bajo coste para múltiples hospitales y carácter modular para poder ser transportado a ubicaciones de campo.},
   author = {David Estevez and Juan G. Victores and Carlos Balaguer},
   journal = {Jornadas Nacionales de Robótica},
   keywords = {computer vision,medical systems,robot vision,robotics,robots},
   title = {HORUS: Inspección robotizada de los trajes de protección del personal sanitario de pacientes en aislamiento de alto nivel, incluido el Ébola},
   year = {2017},
}
@inproceedings{menendez2017autonomous,
   abstract = {This paper presents the ROBO-SPECT European FP7 project, funded under the ICT-2013.2.2 programme on Robotics use cases & Accompanying measures. The main objective of the ROBO-SPECT system is to provide a robotized, faster and reliable alternative to manual tunnel structural inspection and assessment. Physical developments include the design and implementation of a multi-degree-of-freedom (MDoF) robotic system, which uses a mobile vehicle to advance along the roadway, an extended crane capable of reaching the most commonly found tunnel geometries, and a robotic arm for positioning a specifically designed ultrasonic sensor (US) Inspection Tool with high accuracy. A semi-supervised computer vision system to detect tunnel defects, a Ground Control Station (GCS) to provide a Human-Machine Interface (HMI), and an Intelligent Global Controller (IGC) to command the robot and manage communications between the different parts have also been developed. An overview of the fundamental aspects of the project architecture and design will be detailed. In addition, the developed and implemented algorithm for positioning the Tunnel Inspection Tool on detected cracks shall be presented. Finally, experimental evidence to validate the functionality of the ROBO-SPECT system in a real motorway tunnel with ongoing traffic will be provided.},
   author = {Elisabeth Menendez and Juan G. Victores and Roberto Montero and Carlos Balaguer},
   city = {Taipei, Taiwan},
   journal = {Proceedings of the International Symposium on Automation and Robotics in Construction (ISARC)},
   pages = {655-662},
   title = {Autonomous Robotic System with Tunnel Inspection Tool Positioning},
   url = {https://doi.org/10.22260/ISARC2017/0091},
   year = {2017},
}
@article{marinetto2017mobile,
   abstract = {Purpose Intraoperative electron radiation therapy (IOERT) involves the delivery of a high radiation dose during tumor resection in a shorter time than other radiation techniques, thus improving local control of tumors. However, a linear accelerator device is needed to produce the beam safely. Mobile linear accelerators have been designed as dedicated units that can be moved into the operating room and deliver radiation in situ. Correct and safe dose delivery is a key concern when using mobile accelerators. The applicator is commonly fixed to the patient's bed to ensure that the dose is delivered to the prescribed location, and the mobile accelerator is moved to dock the applicator to the radiation beam output (gantry). In a typical clinical set-up, this task is time-consuming because of safety requirements and the limited degree of freedom of the gantry. The objective of this study was to present a navigation solution based on optical tracking for guidance of docking to improve safety and reduce procedure time. Method We used an optical tracker attached to the mobile linear accelerator to track the prescribed localization of the radiation collimator inside the operating room. Using this information, the integrated navigation system developed computes the movements that the mobile linear accelerator needs to perform to align the applicator and the radiation gantry and warns the physician if docking is unrealizable according to the available degrees of freedom of the mobile linear accelerator. Furthermore, we coded a software application that connects all the necessary functioning elements and provides a user interface for the system calibration and the docking guidance. Result The system could safeguard against the spatial limitations of the operating room, calculate the optimal arrangement of the accelerator and reduce the docking time in computer simulations and experimental setups. Conclusions The system could be used to guide docking with any commercial linear accelerator. We believe that the docking navigator we present is a major contribution to IOERT, where docking is critical when attempting to reduce surgical time, ensure patient safety and guarantee that the treatment administered follows the radiation oncologist's prescription.},
   author = {Eugenio Marinetto and Juan G. Victores and Mónica García-Sevilla and Mercedes Muñoz and Felipe Ángel Calvo and Carlos Balaguer and Manuel Desco and Javier Pascau},
   doi = {10.1002/mp.12482},
   issn = {2473-4209},
   issue = {10},
   journal = {Medical Physics},
   keywords = {IOERT,docking,hard docking,image-guided surgery,optical tracking,soft docking},
   pages = {5061-5069},
   title = {Technical Note: Mobile accelerator guidance using an optical tracker during docking in IOERT procedures},
   volume = {44},
   url = {http://dx.doi.org/10.1002/mp.12482},
   year = {2017},
}
@inproceedings{montero2017intelligent,
   abstract = {This paper presents a complete report for the finished ROBO-SPECT European FP7 project, funded under the ICT-2013.2.2 programme on Robotics use cases & Accompanying measures, describing a completely autonomous tunnel inspection robotic system. The document includes a brief introduction about the motivation of the project, defining advantages of robotic inspection over manual methods. The paper contains descriptions of the different components of the robot, including a mobile vehicle, extended crane, high precision robotic arm, computer vision system and ultrasonic sensors. Some details about the software implemented will be discussed as well. Finally, experimental evidence and details on validation in a real motorway tunnel with ongoing traffic will be provided.},
   author = {Roberto Montero and Elisabeth Menendez and Juan G. Victores and Carlos Balaguer},
   doi = {10.1109/ICARSC.2017.7964094},
   isbn = {9781509062331},
   journal = {2017 IEEE International Conference on Autonomous Robot Systems and Competitions, ICARSC 2017},
   pages = {316-321},
   publisher = {IEEE},
   title = {Intelligent robotic system for autonomous crack detection and caracterization in concrete tunnels},
   url = {https://doi.org/10.1109/ICARSC.2017.7964094},
   year = {2017},
}
@inproceedings{fernandez-fernandez2017reducing,
   abstract = {Continuous Goal Directed Actions (CGDA) is a robot learning framework that encodes actions as time series of object and environment scalar features. As the execution of actions is not encoded explicitly, robot joint trajectories are computed through Evolutionary Algorithms (EA), which require a large number of evaluations. The consequence is that evaluations are performed in a simulated environment, and the optimal robot trajectory computed is then transferred to the actual robot. This paper focuses on reducing the number of evaluations required for computing an optimal robot joint trajectory. Particle Swarm Optimization (PSO) methods have been adapted to the CGDA framework to be studied and compared: naíve PSO, Adaptive Fuzzy Fitness Granulation PSO (AFFG-PSO), and Fitness Inheritance PSO (FI-PSO). Experiments have been performed for two representative use cases within CGDA: the 'wax' and the 'painting' action. The experimental results of PSO methods are compared with those obtained with the Steady State Tournament used in the original proposal of CGDA. Conclusions extracted from these results depict a reduction of the number of required evaluations, with simultaneous tradeoff regarding the degree of fulfillment of the objective given by the optimization cost function.},
   author = {Raul Fernandez-Fernandez and David Estevez and Juan G. Victores and Carlos Balaguer},
   doi = {10.1109/ICARSC.2017.7964089},
   isbn = {9781509062331},
   journal = {2017 IEEE International Conference on Autonomous Robot Systems and Competitions, ICARSC 2017},
   pages = {284-289},
   publisher = {IEEE},
   title = {Reducing the number of evaluations required for CGDA execution through Particle Swarm Optimization methods},
   url = {https://doi.org/10.1109/ICARSC.2017.7964089},
   year = {2017},
}
@inproceedings{estevez2017robotic,
   abstract = {There is an increasing demand of robots able to assist people in everyday home tasks. Some of these tasks, specially garment-related tasks (e.g. laundry, ironing, etc), are currently a big challenge for robots due to deformations and the high number of possible configurations the garment can adopt. Ironing, additionally, requires manipulation and control of the iron, as if performed incorrectly it can create even more wrinkles. In this work we present a method for robotic ironing with an unmodified iron and ironing board and a full-body humanoid robot, inspired in the way people perform the ironing task. In this method, the robot performs an ironing action controlled in velocity with feedback from a force/torque sensor. This action is later analyzed by the robot using computer vision to determine if any wrinkle exists, either pre-existent or created by the ironing action. The 3D vision algorithm segments the garment surface to be ironed and computes a Wrinkleness Local Descriptor (WiLD) that determines the location of the wrinkles on the garment. Wrinkles are processed using computer vision techniques on an flattened image created from the WiLD descriptors computed in the prior stage, resulting, if present, in an optimal ironing path that reduces wrinkleness and avoids creating new wrinkles in garments when ironing. The experimental results show that using our method the humanoid robot is able to successfully iron several garments with results similar to the expected from a human performing the same task.},
   author = {David Estevez and Raul Fernandez-Fernandez and Juan G. Victores and Carlos Balaguer},
   doi = {10.1109/ICARSC.2017.7964065},
   isbn = {9781509062331},
   journal = {2017 IEEE International Conference on Autonomous Robot Systems and Competitions, ICARSC 2017},
   pages = {134-139},
   publisher = {IEEE},
   title = {Robotic ironing with a humanoid robot using human tools},
   url = {https://doi.org/10.1109/ICARSC.2017.7964065},
   year = {2017},
}
@incollection{fernandezfernandez2016new,
   abstract = {In this paper, the study and implementation of a task generation system that uses the information obtained from the user and already known cases is presented. One of the main objectives of the system is to introduce a new approach in robotics that takes into account the physical limitation of teaching and learning time, and thus the amount of knowledge that a robot can obtain of a given environment (tasks, objects, user preferences…), as a critical bottleneck of any robotic system. For this, the study of the Case Based Reasoning (CBR) problem is presented. Additionally, Base Trajectory Combination (BATC), a novel trajectory combination method based on a simplified CBR structure, using trajectories instead of high-level tasks, is proposed and explained. Finally, this system is tested with Moveit! as the simulation environment, using the humanoid robot TEO from Universidad Carlos III de Madrid as the robotic platform. The results of these experiments are also presented with the corresponding conclusions and future research lines.},
   author = {Raul Fernandez-Fernandez and Juan G. Victores and Carlos Balaguer},
   city = {Madrid},
   isbn = {978-84-608-8452-1},
   journal = {Robocity16: Open Conference on Future Trends in Robotics},
   keywords = {generation,robot,task},
   month = {5},
   pages = {169-176},
   publisher = {CSIC},
   title = {New Trends and Challenges in the Automatic Generation of New Tasks for Humanoid Robots},
   url = {http://www.robocity2030.org/events/event/evento-esp-2-2/},
   year = {2016},
}
@inproceedings{estevez2016alightweight,
   abstract = {Robotics is being heavily integrated in a great number of daily-life applications, such as domestic assistance, autonomous driving, and healthcare. Other applications, such as entertainment, are beginning to integrate robotic elements. A robotic behavior, however complex, can in general be modelled and implemented as a Finite State Machine (FSM). While these robotic behaviors may typically be implemented using high-level scripting languages such as Lua or Python, or even proprietary solutions, a set of use cases where the efficiency of C++ must be invoked exist. The use case presented in this paper is Robot Devastation, a video-game that combines Augmented Reality elements with robots to emulate combats between robots. Robot Devastation strives for code efficiency to enhance the end-user game play experience. In this work, we present the lightweight reusable C++ library that has been developed for implementing its FSM, reluctantly called StateMachineLib within the rd:: namespace. Aimed at seamless integration with robotic middlewares, let its current loosely coupled integration with the well-known robotic platform YARP serve as a living example its flexibility and usefulness.},
   author = {David Estevez and Juan G. Victores and Carlos Balaguer},
   city = {Cancun},
   journal = {Towards Humanoid Robots OS Workshop at HUMANOIDS 2016},
   keywords = {augmented reality,robot},
   month = {11},
   title = {A Lightweight Finite State Machine C++ Library aimed at Seamless Integration with Robotic Middlewares},
   url = {https://roboticslab-uc3m.github.io/workshop-humanoids2016},
   year = {2016},
}
@inproceedings{loupos2016integrated,
   abstract = {Recent developments in robotic systems, automation and computer vision and sensors have well prepared the ground towards automated robotic solutions for inspection of civil infrastructures and particularly tunnels ageing urgently requiring serious inspection and assessment. Nowadays, tunnel inspections are being performed manually and visually by human operators. ROBO-SPECT is an EC co-funded research project (FP7 - ICT – 611145) that is driven by the tunnel inspection industry and that adapts and integrates recent research results in intelligent control in robotics, computer vision and active continuous learning and sensing, in an innovative, integrated, robotic system. ROBO-SPECT automatically scans the internal surface of tunnels for potential defects (spalling, delamination etc) and inspects and measures radial deformation in the cross-section, distance between parallel cracks, cracks and open joints that impact tunnel stability, with mm accuracies. The robotic system (currently at a prototyping phase) consists of an intelligent robotic system supported by advanced control systems and an on-board high precision robotic tip, an advanced computer vision system based on high quality and stereo cameras for cracks/defects detection and 3D vision, an ultrasonic sensor able to detect crack width and depth with high accuracy. The system also includes a 3D laser scanner able to perform tunnel deformation measurements also with high accuracy. This publication describes the integration status and recent developments of the semi-autonomous robotic system as designed for concrete lining tunnels. The final system architecture, communication mechanisms and integration steps of the whole robotic system are also being presented.},
   author = {Konstantinos Loupos and Angelos Amditis and Anastasios Doulamis and Philippe Chrobocinski and Juan G. Victores and Max Wietek and Panagiotis Panetsos and Alberto Roncaglia and Stephanos Camarinopoulos and Vassileios Kallidromitis and Dimitrios Bairaktaris and Nikolaos Komodakis and Rafa Lopez},
   city = {Cambridge},
   journal = {International Conference on Smart Infrastructure and Construction (ICSIC)},
   keywords = {robot},
   title = {Integrated Robotic Solution for Tunnel Structural Evaluation and Characterization – ROBO-SPECT EC PROJECT},
   url = {http://www.icsic.eng.cam.ac.uk/programme/programme21june/view},
   year = {2016},
}
@incollection{estevez2016anew,
   abstract = {While robots have been recently introduced in domestic environments to perform household chores, few robots are being used for entertainment, partially due to the lack of infrastructures for this purpose. In this paper, we propose an architecture that enables integrating interactive virtual elements in a robot-based gameplay. Several robot game concepts that may be enhanced with Augmented Reality are presented as guidelines toward the future of robot entertainment.},
   author = {David Estevez and Juan G. Victores and Carlos Balaguer},
   city = {Madrid},
   journal = {RoboCity16: Open Conference on Future Trends in Robotics},
   keywords = {augmented reality,robot},
   month = {5},
   pages = {129-136},
   publisher = {CSIC},
   title = {A New Generation of Entertainment Robots Enhanced with Augmented Reality},
   url = {http://www.robocity2030.org/events/event/evento-esp-2-2/},
   year = {2016},
}
@incollection{estevez2016future,
   abstract = {This paper presents current approaches for robotic garment folding-oriented 3D deformable object perception and manipulation. A major portion of these approaches are based on 3D perception algorithms that match garments to a model, and are thus model-based. They require a full view of an extended garment, in order to then apply a preprogrammed folding sequence. Other approaches are based on 3D manipulation algorithms that are focused on modifying the pose of the garment, also oriented at matching it with a model. We present our own garment-agnostic algorithm, which requires no model to unfold clothes, and works using a single view from an RGB-D sensor. The unfolding algorithm also has been validated through experiments using a garment dataset of RGB-D sensor data, and additional validation with a humanoid robot platform. Finally, conclusions regarding the current state of the art and on the future trends of these research lines are discussed.},
   author = {David Estevez and Juan G. Victores and Carlos Balaguer},
   city = {Madrid},
   journal = {RoboCity16: Open Conference on Future Trends in Robotics},
   keywords = {deformable,manipulation,objects,perception,robot,textile},
   month = {5},
   pages = {333-340},
   publisher = {CSIC},
   title = {Future Trends in Perception and Manipulation for Unfolding and Folding Garments},
   url = {http://www.robocity2030.org/events/event/evento-esp-2-2/},
   year = {2016},
}
@inproceedings{estevez2016towards,
   abstract = {Folding clothes is a current trend in robotics. Previously to folding clothes, they have to be unfolded. It is not realistic to perform model-based unfolding, as every garment has a different shape, size, color, texture, etc. In this paper we present a garment-agnostic algorithm to unfold clothes that works using 3D sensor information. The depth information provided by the sensor is converted into a grayscale image. This image is segmented using watershed algorithm. This algorithm provide us with labeled regions, each having a different height. In this labeled image, we assume that the highest height region belongs to the fold. Starting on this region, and ending in the garment border, tentative paths are created in several directions to analyze the height profile. For each profile, a bumpiness value is computed, and the lowest one is selected as the unfolding direction. A final extension on this line is performed to create a pick point on the fold border, and a place point outside the garment. The proposed algorithm is tested with a small set of clothes in different positions.},
   author = {David Estevez and Juan G. Victores and Santiago Morante and Carlos Balaguer},
   doi = {10.1109/ICARSC.2016.65},
   journal = {IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
   keywords = {deformable,laundry,manipulation,object,perception,textile},
   month = {5},
   title = {Towards Robotic Garment Folding: A Vision Approach for Fold Detection},
   url = {https://doi.org/10.1109/ICARSC.2016.65},
   year = {2016},
}
@incollection{morante2015forcesensorless,
   abstract = {In this paper we present two controllers for robots that combine terms for the compensation of gravity forces, and the forces of friction ofmotors and gearboxes. The Low-Friction Zero-Gravity controller allows a guidance of the robot without effort, allowing small friction forces to reduce the free robot motion. It can serve to aid users providing kinesthetic demonstrations while programming by demonstration. In the present, kinesthetic demonstrations are usually aided by pure gravity compensators, and users must deal with friction. A Zero-Friction Zero-Gravity controller results in free movements, as if the robot were moving without friction or gravity influence. Ideally, only inertia drives the movements when zeroing the forces of friction and gravity. Coriolis and centrifugal forces are depreciated. The developed controllers have been tuned and tested for 1 DoF of a full-sized humanoid robot arm.},
   author = {Santiago Morante and Juan G. Victores and Santiago Martínez and Carlos Balaguer},
   doi = {10.1007/978-3-319-27149-1_5},
   isbn = {9783319271484},
   issn = {21945357},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {compensation,friction,gravity,robot},
   publisher = {Springer International Publishing},
   title = {Force-sensorless friction and gravity compensation for robots},
   volume = {418},
   url = {http://doi.org/10.1007/978-3-319-27149-1_5},
   year = {2015},
}
@inproceedings{morante2014on,
   abstract = {In this paper, we study how human-robot interaction can be beneficial on the Continuous Goal-Directed Actions (CGDA) framework. Specifically, a system for robot discovery of motor primitives from random human-guided movements has been developed. These guided motor primitives (GMP) are used as scaffolds to reproduce a goal-directed actions. CGDA encodes goals as the changes produced on object features (color, area, etc) due to actions. This paper focuses on using motor primitives extracted from human-guided random robot movements to execute these goal-directed actions. The human guides the robot joints in random movements, which are later divided in small segments. These segments are compared in terms of joint positions and selected to be diverse. To perform goal-directed actions, the robot must discover an adequate sequence of GMP. To discover these sequences we organize the primitives as a tree with incremental depths (where each node represents a primitive) and use a breadth-first search. In one of the experiments performed, the robot executes a task based on spatial object features. In the other experiment, the goal is to paint a wall by following a color feature trajectory.},
   author = {Santiago Morante and Juan G. Victores and Alberto Jardón and Carlos Balaguer},
   city = {Edinburgh, UK},
   doi = {10.1109/ROMAN.2014.6926320},
   issue = {October},
   journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
   publisher = {IEEE},
   title = {On using guided motor primitives to execute Continuous Goal-Directed Actions},
   volume = {2014-Octob},
   url = {https://doi.org/10.1109/ROMAN.2014.6926320},
   year = {2014},
}
@incollection{montero2015the,
   abstract = {ROBO-SPECT is a European 7th Framework project funded under the ICT programme on Robotics Use Cases (contract No. 611145), implemented by 10 partners from 6 European countries. Driven by the tunnel inspection industry, its main objective is to adapt and integrate recent research results in intelligent control in robotics, computer vision tailored with semisupervised and active continuous learning and sensing, in an innovative, integrated, robotic system that automatically scans tunnel intrados for potential defects on the surface and detects and measures radial deformation in the cross-section, distance between parallel cracks, cracks and open joints that impact tunnel stability.},
   author = {Roberto Montero and Juan G. Victores and Elisabeth Menéndez and Carlos Balaguer},
   city = {Leganés},
   journal = {Robocity2030 13th Workshop EU robotic projects results},
   keywords = {automated,inspection,robot,tunnel},
   pages = {91-100},
   title = {The Robot-Spect EU Project: Autonomous Robotic Tunnel Inspection},
   url = {http://www.robocity2030.org/events/event/13th-robocity2030-workshop/},
   year = {2015},
}
@inproceedings{loupos2015robotic,
   abstract = {Nowadays tunnel structural inspections are executed based on visual (human) inspections. This is a slow process, labor expensive and subjective, while requiring lane/rail shutdown during the inspection. ROBO-SPECT is an EC co-funded research project (FP7 - ICT – 611145) that, driven by the tunnel inspection industry, adapts and integrates recent research results in intelligent control in robotics, computer vision and active learning and sensing, in an integrated, robotic system that automatically scans the intrados of tunnels for potential structural defects. The system additionally inspects and measures radial deformation in the cross-sections, distance between cracks, joints that may impact tunnel stability, with mm accuracies. This publication focuses on ROBO-SPECT EC project first year activities presenting the overall system architecture, as well as the technologies that will be integrated, and overall technological solution. The current status of implementations and the following steps, together with its expected European and International impact is also included.},
   author = {Konstantinos Loupos and Angelos Amditis and Christos Stentoumis and Juan G. Victores and Philippe Chrobocinski and Alberto Roncaglia and Stephanos Camarinopoulos and Nikos Komodakis and Rafael Lopez},
   journal = {Third conference on smart monitoring, assessment and rehabilitation of civil structures (SMAR)},
   title = {Robotic System with Intelligent Vision for Tunnel Structural Assessment - System Architecture – The ROBO-SPECT EC project},
   url = {https://data.smar-conferences.org/SMAR_2015_Proceedings/html/L.html},
   year = {2015},
}
@inproceedings{estevez2015robot-intetain,
   abstract = {We present Robot Devastation, a multiplayer augmented reality game using low-cost robots. Players can assemble their low-cost robotic platforms and connect them to the central server, commanding them through their home PCs. Several low-cost platforms were developed and tested inside the game.},
   author = {David Estevez and Juan G. Victores and Santiago Morante and Carlos Balaguer},
   city = {Turin},
   isbn = {9781479983773},
   journal = {7th International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN)},
   keywords = {augmented reality,diy,game,low-cost,multiplayer interaction,robots},
   month = {6},
   pages = {32-36},
   publisher = {IEEE},
   title = {Robot Devastation: Using DIY Low-Cost Platforms for Multiplayer Interaction in an Augmented Reality Game},
   volume = {15},
   url = {http://ieeexplore.ieee.org/document/7325482/},
   year = {2015},
}
@article{estevez2015robot,
   abstract = {We present Robot Devastation, a multiplayer augmented reality game using low-cost robots. Players can assemble their low-cost robotic platforms and connect them to the central server, commanding them through their home PCs. Several low-cost platforms were developed and tested inside the game.},
   author = {David Estevez and Juan G. Victores and Santiago Morante and Carlos Balaguer},
   doi = {10.4108/icst.intetain.2015.259753},
   isbn = {978-1-63190-061-7},
   issue = {3},
   journal = {EAI Endorsed Transactions on Collaborative Computing},
   keywords = {augmented reality,diy,game,low-cost,multiplayer interaction,robots},
   pages = {1-5},
   publisher = {EAI},
   title = {Robot Devastation: Using DIY Low-Cost Platforms for Multiplayer Interaction in an Augmented Reality Game},
   volume = {15},
   url = {http://dx.doi.org/10.4108/icst.intetain.2015.259753},
   year = {2015},
}
@inproceedings{morante2015automatic,
   abstract = {Robot learning frameworks, such as Programming by Demonstration, are based on learning tasks from sets of user demonstrations. These frameworks, in their naïve implementation, assume that all the data from the user demonstrations has been correctly sensed and can be relevant to the task. Analogous to feature selection, which is the process of selecting a subset of relevant features for use in model construction, this paper presents a demonstration selection process, which is additionally applied for feature selection for further data filtering. The demonstration and feature selection process presented is called Dissimilarity Mapping Filtering (DMF). DMF involves three steps: obtaining a measurement of dissimilarity (e.g. Dynamic Time Warping, etc.), reducing dimensions through a mapping algorithm (e.g. sum of dissimilarities, Multidimen- sional Scaling, etc.) and a filtering method (z-score based, DBSCAN, etc.). As a demonstration selector, DMF discards outlying demonstrations in terms of all the features considered simultaneously. As a feature selector, DMF discards features that present high inconsistency among demonstrations. We apply DMF to our Continuous Goal-Directed Actions (CGDA) robot learning framework presented in previous works.},
   author = {Santiago Morante and Juan G Victores and Carlos Balaguer},
   city = {Seoul},
   doi = {10.1109/HUMANOIDS.2015.7363569},
   journal = {IEEE International Conference on Humanoid Robot - Humanoids},
   keywords = {automatic,demonstration,feature,robot,selection},
   pages = {428-433},
   publisher = {IEEE},
   title = {Automatic Demonstration and Feature Selection for Robot Learning},
   url = {http://dx.doi.org/10.1109/HUMANOIDS.2015.7363569},
   year = {2015},
}
@article{morante2015cryptobotics,
   abstract = {With the expected introduction of robots into our daily lives, providing mechanisms to avoid undesired attacks and exploits in robot communication software is becoming increasingly required. Just as during the beginnings of the computer age (Pfleeger and Pfleeger, 2002), robotics is established in a “happy naivety,” where security rules against external attacks are not adopted, assuming that robotics knowledgeable people are well intended. While this may have been true in the past, the mass adoption of robots will increase the possibilities of attacks. This fact is especially relevant in defense, medical and other critical fields involving humans, where tampering can result in serious bodily harm and/or privacy invasions. For these reasons, we consider that researchers and industry should deploy efforts in cyber safety and acquire good practices when developing and distributing robot software. We propose the term Cryptobotics as a unifying term for research and applications of computer and microcontrollers’ security measures in robotics.},
   author = {Santiago Morante and Juan G. Victores and Carlos Balaguer},
   doi = {10.3389/frobt.2015.00023},
   issue = {23},
   journal = {Frontiers in Robotics and AI},
   keywords = {communications,cryptobotics,cryptography,cyber safety,cyber-physical,robotics},
   pages = {1-4},
   title = {Cryptobotics: Why robots need cyber safety},
   volume = {2},
   url = {http://dx.doi.org/10.3389/frobt.2015.00023},
   year = {2015},
}
@phdthesis{victores2014thesis,
   abstract = {This thesis presents the Robot Imagination System (RIS). This system provides a convenient mechanism for a robot to learn a user's descriptive vocabulary, and how it relates to the world for action. With RIS, a user can describe unfamiliar objects to a robot, and the robot will understand the description as long as it is a combination of words that have been previously used to describe other objects. One of the core uses of the RIS functionality is object recognition. Allowing requests with word combinations that have never been presented before together is well beyond the scope of many of the most relevant state of the art object recognition systems. RIS is not limited to object recognition. Through the use of evolutionary algorithms, the system endows the robot with the capability of generating a mental model (imagination) of a requested unfamiliar object. This capability allows the robot to work with this newly generated model within its simulations, or to expose the model to a user by projecting it on a screen or drawing the mental model as feedback so the user can provide a more detailed description if required. A new paradigm for robot action based on consequences on the environment has been integrated within the RIS architecture. Changes in the environment are continuously tracked, and actions are considered complete when the performed effects are closest to the desired effects, in a closed perception loop. Experimental validations have been performed in real environments using the humanoid robot Teo, bringing the Robot Imagination System closer to everyday household environments in the near future.},
   author = {Juan G. Victores},
   institution = {Universidad Carlos III de Madrid},
   title = {Robot Imagination System},
   url = {http://e-archivo.uc3m.es/handle/10016/19834},
   year = {2014},
}
@article{montero2015past,
   abstract = {Nowadays, the vast majority of the tunnel inspection processes are performed manually by qualified operators. The process is subjective and the operators need to face very uncomfortable and even dangerous conditions such as dust environments, absence of light, or toxic substance exposition among others. Robotic technology can overcome many of these disadvantages and provide quality inspections collecting different types of data. This paper presents the key aspects of tunnel inspection and a survey of the developed robotic tunnel inspection systems up to date. Additionally, two projects regarding automation of the processes involved and future trends will be discussed.},
   author = {Roberto Montero and Juan G. Victores and Santiago Martínez and Alberto Jardón and Carlos Balaguer},
   doi = {10.1016/j.autcon.2015.02.003},
   journal = {Automation in Construction},
   keywords = {Automation,IAARC,Inspection,Maintenance,Robotics,Tunnels},
   pages = {99-112},
   title = {Past, Present and Future of Robotic Tunnel Inspection},
   volume = {59},
   url = {http://dx.doi.org/10.1016/j.autcon.2015.02.003},
   year = {2015},
}
@inproceedings{victores2014on,
   abstract = {Mental models allow us to imagine how something may be, only by its description. This video presents a use-case demonstration of a high-dimensional geometrical system for acquiring mental models for robots, called Robot Imagination System (RIS). RIS generates models of objects based on their descriptive words, even prior to their perception. This is achieved by using an inference algorithm that computes the fusion of features corresponding to descriptive words, allowing to imagine an object whose description has never been presented before. As shown in the video, there is a previous training process where visual data is combined with semantic information. Each keyword creates an n-dimensional instance of the object in the feature space. Feature inference is treated as the intersection of hyperplanes generated from the keywords in the feature space. These hyperplanes extend the meaning of keywords. With a previous basic algorithm, we explored the basis for robotic imagination through mental models. Now, the extended algorithm allows context detection by introducing a previous clustering step combined with cluster bigram co-occurrences, and an analysis of keyword point cloud principal components. In the video, a experiment is performed using the Shortened Token Test.},
   author = {Juan G Victores and Santiago Morante and Alberto Jardón and Carlos Balaguer},
   city = {Madrid},
   doi = {10.1109/HUMANOIDS.2014.7041355},
   journal = {IEEE RAS International Conference on Humanoid Robots (Humanoids 2014)},
   pages = {172},
   publisher = {IEEE},
   title = {On Using Humanoid Robot Imagination to Perform the Shortened Token Test},
   url = {https://youtu.be/KFHFW9dJfzA},
   year = {2014},
}
@inproceedings{morante2014sensorless,
   abstract = {In this video we present a new controller for robots, called Zero-Friction Zero-Gravity control. The controllers combine terms for compensating the forces exerted by gravity, and the forces of friction of motors and reductions. The consequence of applying this control to a robot (a humanoid robot arm in this case) result in a free movement, as if the robot were floating in space. Only the inertia drives the movements by zeroing the forces of friction and gravity. This control, when applied to an adequate robotic platform, could allow to simulate gravity free conditions of movement. A slightly different version of this controller, namely Low-Friction Zero-Gravity control, allows new forms of human-robot interaction. This controller introduces an attenuation variable for the friction compensation term, which makes a smooth and easy interaction with the robot possible, whose movement eventually stops due to the low friction. Experiments are performed using 1 DoF of an arm of the humanoid robot Teo.},
   author = {Santiago Morante and Juan G Victores and Santiago Martinez de la casa and Carlos Balaguer},
   city = {Madrid},
   doi = {10.1109/HUMANOIDS.2014.7041370},
   journal = {IEEE RAS International Conference on Humanoid Robots (Humanoids 2014)},
   pages = {265},
   publisher = {IEEE},
   title = {Sensorless Friction and Gravity Compensation},
   url = {https://www.dailymotion.com/video/x2vjrfs},
   year = {2014},
}
@misc{jardon2014sistema,
   abstract = {La invención proporciona un sistema para la verificación de la trayectoria de un túnel (1). Este sistema comprende, al menos una superficie de recepción adecuada para la recepción de luz láser (3, 13), sensores (4, 14) adecuados para capturar datos sobre la incidencia del láser (9) en la al menos una superficie adecuada para la recepción de luz láser (3, 13), un soporte (5, 15) adecuado para sujetar la al menos una superficie de recepción (3, 13), y medios de procesamiento de información (6). Los sensores (4, 14) están adaptados para transmitir los datos sobre la incidencia del láser (9) a los medios de procesamiento de la información (6), y el soporte (5, 15) puede moverse en varios grados de libertad lineales y/o angulares, estando adaptado para transmitir información sobre su movimiento a los medios de procesamiento de la información (6). La invención también comprende un método de verificación de la trayectoria de un túnel.},
   author = {Alberto Jardón Huete and Santiago Martínez and Juan G. Victores and Carlos Balaguer and Rafael Portero and Marc Martí},
   title = {Sistema y método para la verificación de la trayectoria de un tunel},
   url = {http://invenes.oepm.es/InvenesWeb/detalle?referencia=P201330794},
   year = {2014},
}
@misc{victores2011tool,
   abstract = {Herramienta y método de aplicación automática remota de tiras de cinta con polímero reforzado con fibras, con dispensación de adhesivo epóxico. http://invenes.oepm.es/InvenesWeb/detalle?referencia=PCT/ES2011/000145 The invention relates to a tool (1) and a method for the automatic remote application of strips of non-adhesive fibre-reinforced polymer (FRP) tape (12) for the covering of curved or flat surfaces (20) of construction elements, such as tunnel vaults, parabolic arches, catenary arches and other non-funicular arches typical of bridges and many modern constructions. The tool includes a frame having the following elements coupled thereto, namely: a pressure roller (2); a means for dispensing FRP tape via guide rollers (6a, 6b, 6c, 6d); a drive roller (5); and a means for dispensing adhesive so as to coat said tape, such that by moving the tool over the surface, power is generated and transmitted to an FRP-tape-dispensing means, adjusted such that the FRP dispensing speed is substantially similar to the tool (1) movement speed, thus allowing an automatic remote application method.},
   author = {Juan G. Victores and Santiago Martinez and Alberto Jardón and Carlos Balaguer},
   title = {Tool and method for the automatic remote application of strips of fibre-reinforced polymer tape, comprising the dispensing of epoxy adhesive},
   url = {http://www.google.im/patents/WO2011138481A1?cl=en},
   year = {2011},
}
@inproceedings{loupos2014robotic,
   abstract = {Recent developments in robotics and the associated fields of computer vision and sensors pave the floor for automated robotic solutions, exploitable in the wider field of inspection of civil infrastructures and particularly transportation tunnels, the latter ageing urgently requiring inspection and assessment. Currently, tunnel inspections are performed via visually by human operators. This can result into slow, labour expensive and subjective process often requiring lane shutdown during the inspection, parameters that need to be lowered while having safety requirements and tunnel uptimes increasing. ROBINSPECT is an EC research project (FP7 - ICT – 611145) driven by the tunnel inspection industry, that adapts and integrates recent research results in intelligent control in robotics, computer vision and active continuous learning and sensing, in an innovative, integrated, robotic system that automatically scans the intrados of tunnels for potential defects on the surface while at the same time inspects and measures radial deformation in the cross-section, distance between parallel cracks, cracks and open joints that impact tunnel stability, with mm accuracies. Intelligent control systems and robotics are integrated to set an automatic robotic arm manipulation and autonomous vehicle navigation so as to minimize humans’ interaction during tunnel inspection. This way, the structural condition and safety of a tunnel is assessed automatically, reliably and speedily. The robotic system will be evaluated at the research infrastructure of tunnels of VSH, at three road tunnels of the Egnatia Motorway and the rail tunnel of London Underground. This paper will focus on the ROBISPECT EC project first year activities starting from the requirements, specifications and system architecture as well as the technologies that will be integrated and overall technological solution. Also it will provide the current status of implementations and following steps as well as its expected European and International impact.},
   author = {Konstantinos Loupos and Angelos Amditis and Christos Stentoumis and Philippe Chrobocinski and Juan G. Victores and Max Wietek and Panagiotis Panetsos and Alberto Roncaglia and Stephanos Camarinopoulos and Vassilis Kalidromitis and Dimitris Bairaktaris and Nikos Komodakis and Rafa Lopez},
   city = {Timisoara, Romania},
   journal = {IEEE International Symposium on Robotic and Sensors Environments (ROSE)},
   title = {Robotic Intelligent Vision and Control for Tunnel Inspection and Evaluation - The ROBINSPECT EC Project},
   url = {http://dx.doi.org/10.1109/ROSE.2014.6952986},
   year = {2014},
}
@inproceedings{jardon2014extended,
   abstract = {Microtunneling is a trenchless construction method that is highly adequate to install pipelines beneath roads, rail roads, dams, harbours and environmentally sensitive areas. Microtunneling could be understood as a remotely-controlled, guidance operation of a MicroTunneling Boring Machine (MTBM), where references are provided by a human operator from the surface. A review of current technologies and main working principles of traditional guidance systems used to determine the position and orientation of the drill head of the MTBM during tunneling will be presented. Practical limitations and drawbacks will be discussed. A special attention will be given to those systems based on a laser-generated reference. In this paper a new video target system developed to improve the current performance of guidance systems will be presented. As it will be detailed, improving target processing of laser’ incidence allow us to extend the minimum range to achieve a distance over 400 m, without having to displace the total reference stations which are guiding the path during tunnel execution. The new target sensing principle, the implementation approach, the image processing and pose estimation algorithms will be discussed. Additionally, some preliminary results of the prototype in its current testing phase in real scenarios, gathered in parallel with commercial units, and its comparison will be presented.},
   author = {Alberto Jardón and Santiago Martínez and Juan G. Victores and Carlos Balaguer},
   city = {Sydney, Australia},
   journal = {31st International Symposium on Automation and Robotics in Construction and Mining (ISARC 2014)},
   keywords = {Microtunneling machine,graphical interface,guidance systems,teleoperation},
   pages = {837-846},
   publisher = {University of Technology, Sydney},
   title = {Extended Range Guidance System for the Teleoperation of Microtunnelling Machines},
   url = {http://www.isarc2014.org/pdfs/ISARC2014_Proceedings - New ISBN.pdf},
   year = {2014},
}
@inproceedings{balaguer2014towards,
   abstract = {Tunnels environments are characterized by dust, humidity, and absence of natural light. Artificial and natural impacts, change in load criteria, or the simple effect of ageing, make tunnels require inspection and maintenance. These operations are commonly performed by human workers taking time and expertise without guarantee quality control. Robotic tunnel inspection and maintenance (RTIM) introduces high productivity, quality and repetitiveness. This paper describes the current trends in the subject, and introduces new technologies such as scenario modeling, robotic platforms, image and ultrasound sensors, control algorithms and decision making strategies. Additionally, the result of several recent and ongoing projects will be presented.},
   author = {Carlos Balaguer and Roberto Montero and Juan G. Victores and Santiago Martínez and Alberto Jardón},
   city = {Sydney, Australia},
   editor = {Quang Ha and Xuesong Shen and Ali Akbarnezhad},
   journal = {Proceedings of the 31st International Symposium on Automation and Robotics in Construction and Mining (ISARC 2014)},
   keywords = {Automation,IAARC,Inspection,Maintenance,Robotics,Tunnels},
   pages = {19-33},
   publisher = {University of Technology, Sydney},
   title = {Towards Fully Automated Tunnel Inspection: A Survey and Future Trends},
   url = {http://www.isarc2014.org/pdfs/ISARC2014_Proceedings - New ISBN.pdf},
   year = {2014},
}
@article{victores2014an,
   abstract = {In this paper, we present an accessible interface in the context of our work on bringing advanced robotics closer to everyday domestic users. This interface allows inexperienced users to be capable of programming an assistive robotic arm to perform a specific desired task in a household environment. The programming process is performed through the developed Web Browsable interface, within which a Task Creator Wizard plays an essential role. The robot's open architecture enables flexible multi-modal interaction. In addition to the touch buttons provided by the Web Browsable interface when presented on a touch screen, voice commands and the use of the Wii RemoteTM controller for intuitive robotic movement have also been enabled. The Web Browsable interface has been designed to provide high accessibility while taking aesthetic details into account, in order to prevent distraction caused by boredom of the user.},
   author = {Juan G. Victores and Santiago Morante and Alberto Jardón and Carlos Balaguer},
   doi = {10.17411/jacces.v4i3.49},
   issn = {2013-7087},
   issue = {3},
   journal = {Journal of Accessibility and Design for All (JACCES)},
   keywords = {assistive robot,graphical interface,usability,user-},
   pages = {161-176},
   title = {An Accessible Interface For Programming An Assistive Robot},
   volume = {4},
   url = {http://dx.doi.org/10.17411/jacces.v4i3.49},
   year = {2014},
}
@article{morante2015humanoid,
   abstract = {Humanoids can learn motor skills through the programming by demonstration framework, which allows matching the kinematic movements of a robot with those of a human. Continuous goal-directed actions (CGDA) is a framework that can complement the paradigm of robot imitation. Instead of kinematic parameters, its encoding is centered on the changes an action produces on object features. The features can be any measurable characteristic of the object such as color, area, etc. The execution of actions encoded as CGDA allows a robot-configuration independent achievement of tasks, avoiding the correspondence problem. By tracking object features during action execution, we create a trajectory in an n-dimensional feature space that represents object temporal states, allowing generalization, recognition, and execution of action effects on the environment. Experiments have been performed, using a humanoid robot in a simulated environment. Evolutionary computation was used for joint parameter calculation of a humanoid robot. The objective is to generate a motor trajectory which leads to a feature trajectory equal to the objective one. In one of the experiments, the robot performs a spatial trajectory based on spatial object features. In a new experiment, the robot paints a wall by following a color feature encoding.},
   author = {Santiago Morante and Juan G. Victores and Alberto Jardón and Carlos Balaguer},
   doi = {10.1080/01691864.2014.964314},
   issn = {1568-5535},
   issue = {Special issue: Humanoid robots},
   journal = {Advanced Robotics},
   keywords = {computation,evolutionary,goal-directed imitation,learning from demonstration,motor primitives,robot learning},
   pages = {303-314},
   title = {Humanoid Robot Imitation through Continuous Goal-Directed Actions: An Evolutionary Approach},
   volume = {29},
   url = {http://dx.doi.org/10.1080/01691864.2014.964314},
   year = {2015},
}
@article{crespo2014algorithm,
   abstract = {Visibility graphs are basic planning algorithms, widely used in mobile robotics and other disciplines. The construction of a visibility graph can be considered a tool based on geometry that provides support to planning strategies in mobile robots. Visually, the method is used to solve that planning, which is quite extended due to the simplicity of operating with polygons, that represent obstacles in the environment. The cost of these algorithms tend to be quite low. The most sensitive issue of obtaining visibility between polygons is in cases in which the polygons are non-convex. In such cases, it is obligatory to know whether the area where one vertex of the polygon is found, is located in a convex or non-convex area, being desirable to distinguish between both situations in a simple way, issue that was not possible up to now. To obtain the visibility of non-convex polygons, the authors have developed a visual and intuitive method which gives the machine the ability to interpret the visibility with a simplicity similar to the human mind.},
   author = {Jonathan Crespo and Ramon Barber and Juan G. Victores and Alberto Jardón},
   issn = {2278-0149},
   issue = {2},
   journal = {Journal of Mechanical Engineering and Robotics Research},
   pages = {150-170},
   title = {Algorithm for Graph Visibility Obtainment from a Map of Non-Convex Polygons},
   volume = {3},
   url = {http://www.ijmerr.com/v3n2/ijmerr_v3n2_19.pdf},
   year = {2014},
}
@incollection{jardon2014a,
   author = {Alberto Jardón and Félix R. Cañadillas and Juan G. Victores and Santiago Martínez and Carlos Balaguer},
   doi = {10.1007/978-3-319-03653-3_4},
   editor = {Manuel A. Armada and Alberto Sanfeliu and Manuel Ferre},
   isbn = {978-3-319-03652-6},
   journal = {ROBOT2013: First Iberian Robotics Conference},
   keywords = {This paper presents a review of CEABOT,a brief review of some didactic approaches experie,a review of similar contests,and some insights about the future of this and sim,objectives and results will be discussed. A brief,or tune up a commercial kit. The challenge is to o,the unique humanoids competition for university st,they have to program the sensor and motor skills o},
   pages = {41-52},
   publisher = {Springer International Publishing},
   title = {A Review of Eight Years of CEABOT Contest: A National Wide Mini Humanoids Competition},
   url = {http://dx.doi.org/10.1007/978-3-319-03653-3_4},
   year = {2014},
}
@inproceedings{stoelen2013adaptive,
   abstract = {An approach for adaptive shared control of an assistive manipulator is presented. A set of distributed collision and proximity sensors is used to aid in limiting collisions during direct control by the disabled user. Artificial neural networks adapt the use of the proximity sensors online, which limits movements in the direction of an obstacle before a collision occurs. The system learns by associating the different proximity sensors to the collision sensors where collisions are detected. This enables the user and the robot to adapt simultaneously and in real-time, with the objective of converging on a usage of the proximity sensors that increases performance for a given user, robot implementation and task-set. The system was tested in a controlled setting with a simulated 5 DOF assistive manipulator and showed promising reductions in the mean time on simplified manipulation tasks. It extends earlier work by showing that the approach can be applied to full multi-link manipulators.},
   author = {Martin F. Stoelen and Virginia F. Tejada and Juan G. Victores and Alberto Jardón and Fabio Bonsignorio and Carlos Balaguer},
   doi = {10.1109/IROS.2013.6696494},
   journal = {Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2013)},
   pages = {1143-1148},
   title = {Adaptive collision-limitation behavior for an assistive manipulator},
   url = {http://dx.doi.org/10.1109/IROS.2013.6696494},
   year = {2013},
}
@inproceedings{morante2014action,
   abstract = {Programming by demonstration (PbD) allows matching the kinematic movements of a robot with those of a human. The presented Continuous Goal-Directed Actions (CGDA) is able to additionally encode the effects of a demonstrated action, which are not encoded in PbD. CGDA allows generalization, recognition and execution of action effects on the environment. In addition to analyzing kinematic parameters (joint positions/velocities, etc.), CGDA focuses on changes produced on the object due to an action (spatial, color, shape, etc.). By tracking object features during action execution, we create a trajectory in an n-dimensional feature space that represents object temporal states. Discretized action repetitions provide us with a cloud of points. Action generalization is accomplished by extracting the average point of each sequential temporal interval of the point cloud. These points are interpolated using Radial Basis Functions, obtaining a generalized multidimensional object feature trajectory. Action recognition is performed by comparing the trajectory of a query sample with the generalizations. The trajectories discrepancy score is obtained by using Dynamic Time Warping (DTW). Robot joint trajectories for execution are computed in a simulator through evolutionary computation. Object features are extracted from sensors, and each evolutionary individual fitness is measured using DTW, comparing the simulated action with the generalization.},
   author = {Santiago Morante and Juan G. Victores and Alberto Jardón and Carlos Balaguer},
   city = {Hong Kong, China},
   doi = {10.1109/ICRA.2014.6907098},
   journal = {ICRA IEEE International Conference on Robotics and Automation},
   publisher = {IEEE},
   title = {Action Effect Generalization, Recognition and Execution through Continuous Goal-Directed Actions},
   url = {http://dx.doi.org/10.1109/ICRA.2014.6907098},
   year = {2014},
}
@incollection{victores2014assistive,
   abstract = {This paper presents a multi-modal interface for interaction between people with physical disabilities and an assistive robot. This interaction is performed through a dialogue mechanism and augmented 3D vision glasses to provide visual assistance to an end user commanding an assistive robot to perform Daily Life Activities (DLAs). The augmented 3D vision glasses may provide augmented reality vision of menus and information dialogues over the view of the real world, or in a simulator environment for laboratory tests and user evaluation. The actual dialogue is implemented as a finite state machine, and includes possibilities of Automatic Speech Recognition (ASR), and a Text-to-Speech (TTS) converter. The final study focuses on studying the effectiveness of these visual and auditory aids for enabling the end user to command the assistive robot ASIBOT to perform a given task.},
   author = {Juan G. Victores and Félix R. Cañadillas and Santiago Morante and Alberto Jardón and Carlos Balaguer},
   city = {Madrid},
   doi = {10.1007/978-3-319-03413-3_15},
   editor = {Manuel A. Armada and Alberto Sanfeliu and Manuel Ferre},
   isbn = {978-3-319-03412-6},
   journal = {ROBOT2013: First Iberian Robotics Conference},
   keywords = {assistive robotics,augmented reality,end-user development,human-robot interaction,multi-modal interacion,speech recognition},
   pages = {209-217},
   publisher = {Springer International Publishing},
   title = {Assistive Robot Multi-modal Interaction with Augmented 3D Vision and Dialogue},
   url = {http://dx.doi.org/10.1007/978-3-319-03413-3_15},
   year = {2014},
}
@inproceedings{jardon2011aplicacion,
   abstract = {La interacción humano-robot (HRI) en robótica asistencial tiene un varias características importantes que la distingue de otras formas de interacción. Esto incluye la necesidad de gran flexibilidad, seguridad y confianza en el control del sistema robótico. Interpretar el sistema como un binomio humano-robot, con el usuario y el robot actuando dentro de un bucle de control cerrado, puede ser beneficioso para comprender y mejorar la interacción. Este documento investiga la viabilidad del modelado y la cuantificación de HRI en robótica asistencial teniendo en cuenta este binomio y aplicando conceptos de la teoría de la información (IT), así como la implementación de un entorno virtual que permita la validación experimental de las posibles mejoras.},
   author = {Alberto Jardón and Martin F Stoelen and Virginia Fernández and Juan G. Victores and Santiago Martínez and Carlos Balaguer and Fabio Bonsignorio},
   city = {Madrid. Spain},
   journal = {IV Congreso Internacional de Diseño, Redes de Investigación y Tecnología para todos (DRT4ALL)},
   pages = {36-45},
   publisher = {Fundación ONCE},
   title = {Aplicación de teoría de la información para el modelado y cuantificación de la interacción persona-robot},
   url = {http://www.discapnet.es/Castellano/areastematicas/tecnologia/DRT4ALL/ES/DRT4ALL2011/Documents/Libro-de-actas-congreso-DRT4all-2011.pdf},
   year = {2011},
}
@inproceedings{rodriguez2013diseno,
   abstract = {En este artículo se presenta el diseño preliminar de una interfaz de interacción entre personas con discapacidad física y un robot asistencial. Esta interfaz proporcionará un lienzo en blanco al usuario, de tal forma que mediante futuras iteraciones se proporcionarán las funcionalidades que el usuario determine necesarias para el sistema. Por tanto, este proyecto se basa en el desarrollo de una interfaz humano-robot que proporcione el mayor número de posibilidades a la hora de realizar dicha interacción. Para ello, se ha determinado utilizar un sistema combinando realidad aumentada, reconocimiento y síntesis de voz. Posteriormente, se muestra la arquitectura propuesta para tal interacción, explicando las herramientas utilizadas para el desarrollo. Por último, se describirá un caso de uso del sistema, en el cual se detalla un posible procedimiento para realizar una tarea concreta.},
   author = {Félix R. Cañadillas and Alberto Jardón and Carlos Balaguer and Juan G. Victores},
   journal = {V Congreso Internacional de Diseño, Redes de Investigación y Tecnología para todos (DRT4ALL)},
   pages = {213-219},
   title = {Diseño Preliminar de Interfaces de Realidad Aumentada para el Robot Asistencial ASIBOT},
   url = {http://www.discapnet.es/Castellano/areastematicas/tecnologia/DRT4ALL/EN/DRT4ALL1013/Paginas/Introduction.aspx},
   year = {2013},
}
@inproceedings{jardon2012extended,
   author = {Alberto Jardón and Santiago Martínez and Juan G. Victores and M. Marti and Carlos Balaguer},
   journal = {International Symposium for Automation and Robotics in Construction 2012 (ISARC/Gerontechnology 2012)},
   title = {Extended range guidance system for micro-tunnelling machine},
   url = {http://gerontechnology.info/index.php/journal/article/view/1814},
   year = {2012},
}
@inproceedings{jardon2012experimental,
   author = {Alberto Jardón and Juan G. Victores and Martin F Stoelen and Santiago Martínez and Carlos Balaguer},
   journal = {International Symposium for Automation and Robotics in Construction (ISARC/Gerontechnology 2012)},
   title = {Experimental evaluation of assistive robots in virtual domestic scenarios},
   url = {http://www.gerontechnology.info/index.php/journal/article/view/gt.2012.11.02.489.00},
   year = {2012},
}
@inproceedings{gonzalezgomez2011motion,
   abstract = {The motion of wheeled mobile robots is inherently based on their wheels' rolling capabilities. The assumption is that each wheel can rotate indefinitely, backwards or forward. This is the starting point for all motion control mechanisms of wheeled robots. In this paper, a new motion capability of differential mobile robots with limited wheel rotation capabilities is presented. The robot will be able to travel any distance and change its direction of movement even if its wheels can not rotate within more than a certain range of angles. The proposed solution is based on the bio-inspired controller principles used for modular and legged robots, in which oscillations are generated for achieving motion. A total of two oscillators, one per wheel, are enough to generate well-coordinated rhythms on the wheels to control the robot motion. The kinematics of this new type of mobile robot motion is presented, and the relation between the oscillator's parameters and the trajectory is studied. Experiments with real robots will demonstrate the viability of this new locomotion gait.},
   author = {Juan Gonzalez-Gomez and Juan G. Victores and Alberto Valero-Gomez and Mohammed Abderrahim},
   doi = {10.1109/ROBIO.2011.6181351},
   journal = {IEEE International Conference on Robotics and Biomimetics (ROBIO)},
   keywords = {Joints,Mobile robots,Oscillators,Robot kinematics,Trajectory,Wheels,bio-inspired controller principles,bio-inspired materials,coordinated rhythms,differential wheeled robots,joint limit constraints,legged locomotion,locomotion gait,motion control,oscillator parameters,robot motion,wheel rotation,wheeled mobile robots},
   pages = {596-601},
   title = {Motion control of differential wheeled robots with joint limit constraints},
   url = {http://dx.doi.org/10.1109/ROBIO.2011.6181351},
   year = {2011},
}
@inproceedings{gonzalezfierro2013object,
   abstract = {In this work, we present a method to tag objects by applying a color model learned from another source object. We learn the statistical color model of objects using Gaussian Mixture Models and Expectation-Maximization algorithm. The source model is transferred to the target object to be tagged by matching the Gaussian distribution that best describe the color struc- ture. This makes the target gain the color model of the source while main- taining its initial appearance. This algorithm can be used in Human-Robot Interaction to visually tag objects for selection, targeting or discrimination. We perform some experiments to test our proposed method.},
   author = {Miguel González-Fierro and Miguel Angel Maldonado and Juan G. Victores and Santiago Morante and Carlos Balaguer},
   journal = {Robocity2030 12th Workshop Robotica Cognitiva},
   title = {Object Tagging for Human-Robot Interaction by Recolorization Using Gaussian Mixture Models},
   url = {https://www.researchgate.net/publication/269631653_Object_Tagging_for_Human-Robot_Interaction_by_Recolorization_Using_Gaussian_Mixture_Models},
   year = {2013},
}
@inproceedings{victores2013semantic,
   abstract = {This paper presents the initial steps towards a robot imagination system, which aims at providing robots with the cognitive capability of imagining how a set of actions can affect a robot's environment, even if the robot has never seen the specific set of actions applied to its environment before. This robot imagination system is part of a human-inspired and goal- oriented infrastructure, which first learns the semantics of actions by human demonstration, and is then capable of performing the inverse semantic reconstruction process through mental imagery. A key factor in this system is distinguishing how different actions affect different features of objects in the environment. Simple probabilistic and other machine learning methods, tested to perform this first step of the inference process, are presented and compared in this paper. The inference of results of composed actions is generated as the sum of the contributions of each of the query word components. As an initial prototype, the actual learning process has been performed using synthetically created minimalistic environments as datasets, and a limited amount of training words for the learning process.},
   author = {Juan G. Victores and Santiago Morante and Alberto Jardón and Carlos Balaguer},
   journal = {Robocity2030 12th Workshop Robotica Cognitiva},
   pages = {35-46},
   title = {Semantic Action Parameter Inference through Machine Learning Methods},
   url = {https://www.researchgate.net/publication/269631454_Semantic_Action_Parameter_Inference_through_Machine_Learning_Methods},
   year = {2013},
}
@inproceedings{victores2013creacion,
   author = {Juan G. Victores and Santiago Morante and Alberto Jardón and Carlos Balaguer},
   journal = {Congreso Iberoamericano de Tecnologias de Apoyo a la Discapacidad Iberdiscap},
   keywords = {asibot,brazo manipulador,desarrollo orientado a usuarios},
   pages = {21-24},
   title = {Creación de Tareas de Asistencia Robótica Mediante la Interacción Multimodal},
   url = {http://iberdiscap2013.pucmm.edu.do/programa},
   year = {2013},
}
@inproceedings{victores2013towards,
   abstract = {This paper presents a robot imagination system that generates models of objects prior to their perception. This is achieved through a feature inference algorithm that enables computing the fusion of keywords which have never been presented to the robot together previously. In this sense, robot imagination is defined as the robot's capability of generating feature parameter values of unknown objects by generalizing characteristics from previously presented objects. The system is first trained with visual information paired with semantic object descriptions from which keywords are extracted. Each keyword creates an instance of the learnt object in an n-dimensional feature space. The core concept behind the robot imagination system presented in this paper is the use of statistically fit hyperplanes in the feature space to represent and simultaneously extend the meaning of grounded words. The inference algorithm allows to determine complete solutions in the feature space. Finally, evolutionary algorithms are used to return these numeric values to the real world, completing an inverse semantic process.},
   author = {Juan G. Victores and Santiago Morante and Alberto Jardón and Carlos Balaguer},
   city = {Tokyo, Japan},
   doi = {10.1109/IROS.2013.6697181},
   journal = {IROS IEEE/RSJ International Conference on Intelligent Robots and Systems},
   pages = {5694-5699},
   publisher = {IEEE},
   title = {Towards Robot Imagination Through Object Feature Inference},
   url = {http://dx.doi.org/10.1109/IROS.2013.6697181},
   year = {2013},
}
@inproceedings{victores2013give,
   abstract = {In this paper we illustrate our work focusing on bringing advanced robotics closer to everyday domestic users. It will be demonstrated that inexperienced users can be capable of programming the ASIBOT assistive robot platform to perform a specific desired task in a household environment. The process is guided through the robot's Web browsable interface Task Creator Wizard. The robot's open architecture has been developed to enable flexible multi-modal interaction, such as the used touch buttons, voice commands, and Wii RemoteTM controller for intuitive robotic movement. The Wizard has been designed to provide enhanced accessibility while taking aesthetic beauty into account, to avoid distraction caused by boredom of the user. The whole concept and implementation has been released as part of the ASIBOT Open Source Code Repository, available online for download and documentation at: http://roboticslab.sourceforge.net/asibot},
   author = {Juan G. Victores and Santiago Morante and Alberto Jardón and Carlos Balaguer},
   journal = {V Congreso Internacional de Diseño, Redes de Investigación y Tecnología para todos (DRT4ALL)},
   title = {“Give me the Red Can”: Assistive Robot Task Creation through Multi-Modal Interaction},
   url = {http://www.discapnet.es/Castellano/areastematicas/tecnologia/DRT4ALL/EN/DRT4ALL1013/Paginas/Introduction.aspx},
   year = {2013},
}
@inproceedings{victores2013augmented,
   abstract = {Gaming environments have proved to be very popular and easily accessi- ble social interaction platforms in a nowadays highly technological and sometimes isolated world. Parallel to this, robotics has tried over the years to provide educational tools to reach closer to younger generations. With this aim in mind, this paper presents the fusion between massively highly distributed video-games and real interaction with actual physical robot platforms, both commercial and amateur ones. The resulting development is a network distributed game called Robot Devastation, which calls for amateurs and professional roboticists to collaborate, play and develop software, hardware and algorithms crossing the bridge between the virtual and real world, making use of cutting-edge technologies like augmented reality and cloud computing.},
   author = {Juan G. Victores and Santiago Morante and Miguel González-Fierro and Carlos Balaguer},
   journal = {Robocity2030 11th Workshop Robots Sociales},
   keywords = {Gaming environments have proved to be very popular,both commercial and amateur ones. The resulting de,hardware and algorithms crossing the bridge betwee,making use of cutting-edge technologies like augme,play and develop software,robotics has tried over the years to provide educa,this paper presents the fusion between massively h,which calls for amateurs and professional robotici},
   pages = {131-143},
   title = {Augmented reality and social interaction platform through multirobot design},
   url = {https://www.researchgate.net/publication/269630253_Augmented_reality_and_social_interaction_platform_through_multirobot_design},
   year = {2013},
}
@article{martinez2013flexible,
   abstract = {Purpose – The paper aims to present the concept, the layout design and the evaluation performed of a flexible field factory for construction industry. Both the concept and layout are focused on flexibility and mobility factors, providing a versatile system for manufacturing and assembly that can be transported to construction sites without need of special permissions. Design/methodology/approach – The design is based on the design for manufacture and assembly (DFMA) principles, lean manufacturing, and construction industry experts' knowledge. Findings – The developed factory layout is dimensioned to fit in a standard 20-feet-long container. Simulation processes have been run to verify the viability of the system. The time estimates calculated in the simulations are compared with traditional in and off-site construction method estimates, providing quantified cost and time benefits. Originality/value – This paper presents the concept of the robotized field factory designed for on-site prefabrication, the design of which began during the EU 6FP ManuBuild Project. This reconfigurable and flexible system is oriented to the production of small and medium size modular systems. The viability of the field factory has been evaluated thanks to the application of a modular system for building installations called Service Core. Its design has been based on DFMA and lean principles as well as the expertise from construction partners from the ManuBuild Project.},
   author = {Santiago Martínez and Alberto Jardón and Juan G. Victores and Carlos Balaguer},
   doi = {10.1108/01445151311306708},
   issue = {2},
   journal = {Assembly Automation},
   keywords = {Building industrialisation,DFMA,Lean manufacturing,Lean production,Modular assembly,Robotics},
   pages = {175-183},
   title = {Flexible Field Factory for Construction Industry},
   volume = {33},
   url = {http://dx.doi.org/10.1108/01445151311306708},
   year = {2013},
}
@inproceedings{stoelen2011methodologies,
   author = {Martin F Stoelen and Alberto Jardón and Virginia Fernández and Juan G. Victores and Santiago Martinez and Fabio Bonsignorio and Carlos Balaguer},
   city = {Madrid. Spain},
   isbn = {978-84-614-5558},
   journal = {Robocity2030 9th Workshop. Robots colaborativos e interacción humano-robot},
   title = {Methodologies for Experimental Evaluation of Assistive Robotics HRI},
   url = {https://www.researchgate.net/publication/269574315_Methodologies_for_Experimental_Evaluation_of_Assistive_Robotics_HRI},
   year = {2011},
}
@inproceedings{victores2010ratim,
   author = {Juan G. Victores and Santiago Martinez and Alberto Jardón and Carlos Balaguer},
   city = {Arganda del Rey. Spain.},
   isbn = {978-84-614-5558},
   journal = {Robocity2030 8th Workshop. Robots de Exteriores.},
   pages = {373-382},
   title = {R.A.T.I.M. Sistema Robótico de Inspección y Mantenimiento de Túneles},
   url = {https://www.researchgate.net/publication/269574005_RATIM_Sistema_Robtico_de_Inspeccin_y_Mantenimiento_de_Tneles},
   year = {2010},
}
@inproceedings{victores2010asibot,
   abstract = {Robotic devices are progressively being incorporated into our everyday lives. Advanced robot technology has been making its move from the dull, stationary regime of production plants and industries to consumer stores, and finally into our own homes. From automatic window shades to motor- ized vacuum cleaning units, these technologies are constantly being intro- duced to make our everyday lives easier. Many still live unaware of how deeply we have been submerged into a world of automatic systems which help us perform everyday tasks, starting by the simplest ones. Moreover, as technology advances, more complex systems are being incorporated in- to our daily lives. Current world-wide research focuses on how to intro- duce dynamic and mobile elements to perform “household chores”, chores that require dexterous manipulation and advanced sensing and reasoning. This is a huge objective that implies great improvement and advances in current robotic technologies related to 24 hour availability, safety, and user satisfaction. As a consequence, there is an imminent need to develop per- ception mechanisms that offer a sufficient degree of reliability for the end- user. These mechanisms may be based on visual, tactile, or other types of sensed information. Additionally, they must be integrated within fully functional robotic systems that comply with the user’s needs. A perception mechanism based on camera vision has been developed for the ASIBOT assistive robot. Recognition and identification is performed through color segmentation, whereas localization is achieved using non-linear interpola- tions on data based on predefined look-up tables. The entire system has been implemented and is being tested on the ASIBOT-based Domestic Aided Kitchen test-bed.},
   author = {Juan G. Victores and Alberto Jardón and Martin F Stoelen and Santiago Martinez and Carlos Balaguer},
   city = {Móstoles. Spain.},
   isbn = {84-693-6777-3},
   journal = {Robocity2030 7th Workshop Visión en Robótica},
   title = {ASIBOT Assistive Robot with Vision in a Domestic Environment},
   url = {https://www.researchgate.net/publication/269573982_ASIBOT_assistive_robot_with_vision_in_a_domestic_environment},
   year = {2010},
}
@inproceedings{stoelen2010towards,
   abstract = {The development of assistive robots for elderly and disabled people is currently an active field of research in the robotics community. An important part of making these systems usable is to allow for multimodal Human-Robot Interaction (HRI). However, the overall human-machine system is complex. The user and the robot are operating in a closed loop and both are potentially capable of adapting to the other. The work presented here has attempted to approach the problem from three different perspectives, investigating methods for analyzing, implementing, and testing an enabling multimodal interface for the ASIBOT assistive robot. It was proposed to use principles from Information Theory as the basis for the analysis, with the goal of increasing the information capacity of the human-machine channel. Multimodality was identified as one possible approach for achieving this. Methods for performing information fusion and machine learning that might be of interest for the implementation were identified. It was speculated that reinforcement learning could serve as an on-line adaptive component in the interface. Finally, the use of standard movement models and tasks as the basis for testing multimodal HRI was discussed and linked to typical tasks for assistive robots.},
   author = {Martin F Stoelen and Alberto Jardón and Fabio Bonsignorio and Juan G. Victores and Concepción Monje and Carlos Balaguer},
   city = {Anchorage, AK. USA},
   journal = {Workshop on Mutimodal Human-Robot Interfaces, IEEE International Conference on Robotics and Automation (ICRA)},
   title = {Towards an Enabling Multimodal Interface for an Assistive Robot},
   url = {https://www.researchgate.net/publication/269573950_Towards_an_Enabling_Multimodal_Interface_for_an_Assistive_Robot},
   year = {2010},
}
@inproceedings{victores2010benchmarking,
   abstract = {Usability can be defined as the degree of a product’s fitting to the characteristics of a person or of a group of people. The concept of usability includes aspects of using a product that are closely linked to the user’s degree of satisfaction and preferences. As a multidisciplinary concept, definitions may vary depending upon the specific area on which one focuses. However, common terms can be found throughout literature. Parameters such as the difficulty and steepness of the learning curve for the end-user, or flexibility and adaptability are commonly evaluated. In the context of Assistive Robots, factors taken into account are related to user acceptance, security, precision of task execution, and overall system efficiency. Hence, it is also closely linked to the concept of dependability. Boundary conditions related to the environment and to the user must be taken into account. In this paper, the importance of the role of benchmarking the usability of Assistive Robots is discussed, and a methodology for obtaining usability data from experiments is proposed. The proposed methodology is part of a continuous improvement framework that is based on the System Knowledge Space, which will be described within the text. Then, a general view at results extracted from experiments performed with an Assistive Robot and real potential system end-users in realistic scenarios is given. This exemplary usability benchmarking assessment follows the guidelines of the methodology that is proposed. The experiments that are described were developed as part of the ASIBOT program at the Universidad Carlos III de Madrid in collaboration with the National Paraplegic Hospital in Toledo (Hospital Nacional de Parapléjicos de Toledo). The last part of our paper deals with results of how these experiences have influenced actual and future research efforts and discusses how this should positively affect the scientific research and developer community.},
   author = {Juan G. Victores and Alberto Jardón and Fabio Bonsignorio and Martin F Stoelen and Carlos Balaguer},
   city = {Anchorage, AK. USA.},
   journal = {Workshop on the Role of Experiments in Robotic Research, IEEE International Conference on Robotics and Automation (ICRA)},
   title = {Benchmarking Usability of Assistive Robotic Systems: Methodology and Application},
   url = {http://www.heronrobots.com/EuronGEMSig/downloads/Anchorage/ICRA10-experiments_final_uc3m.pdf},
   year = {2010},
}
@inproceedings{victores2011interaccion,
   abstract = {Las tecnologías modernas se están incorporando progresivamente a nuestra vida cotidiana, encontrándonos rodeados de elementos que están compuestas por avanzados sistemas electrónicos embebidos: teléfonos móviles, ebooks, netbooks. Son ordenadores en miniatura que poco se parecen a sus antecesores, que ocupaban habitaciones enteras y sólo podían ser manejados por especialistas. Incluso niños pequeños son capaces de interactuar con sus pantallas táctiles o botones, navegando a través de pestañas, menús, e iconos. Lo que es más, incluyen la electrónica y el software necesarios para proporcionar las capacidades de comunicación inalámbrica que son explotadas por los autores, como se explica en el contenido de este capítulo. Todos estos avances han sido incorporados de forma natural en el ámbito de la robótica. Desde el régimen estacionario de las plantas de producción, ahora esta tecnología se puede encontrar en tiendas de consumo, y en última instancia, en nuestro entorno doméstico. En el amplio alcance de la robótica y la automatización, esto incluye desde persianas automáticas y aspiradoras motorizadas a manipuladores robóticos avanzados menos habituales. La investigación mundial actual se centra en cómo introducir elementos dinámicos y móviles para realizar “tareas domésticas” que requieren manipulación compleja y habilidades de razonamiento. Sin embargo, estas tecnologías empezarán a hacer nuestra vida más sencilla sólo con el desarrollo de interfaces humano-robot que proporcionen comodidad y satisfacción al usuario. En este capítulo, los autores proponen la fusión de la robótica con las capacidades de navegación web de los dispositivos modernos, y presentan implementaciones de prueba de concepto para tres plataformas robóticas diferentes: un robot móvil, un UAV, y un robot manipulador escalador.},
   author = {Juan G. Victores and Alberto Jardón and Santiago Morante and Martin F Stoelen and Santiago Martinez and Carlos Balaguer},
   city = {Madrid},
   isbn = {978-84-7484-238-8},
   journal = {Robocity2030 9th Workshop, Robots colaborativos e interaccion humano-robot},
   pages = {75-92},
   title = {Interacción humano-robot a través de interfaces en la nube},
   url = {https://www.researchgate.net/publication/269574247_Interaccion_humano-robot_a_traves_de_interfaces_en_la_nube},
   year = {2011},
}
@inproceedings{balaguer2011sultan,
   author = {Carlos Balaguer and Alberto Jardón and Concepción Monje and Fabio Bonsignorio and Martin F Stoelen and Santiago Martínez and Juan G. Victores},
   city = {San Francisco},
   editor = {Kazuyoshi Wada and Machiel Van der Loos and Loredana Zollo},
   journal = {Workshop on New and Emerging Technologies in Assistive Robotics at IROS 2011},
   pages = {6-8},
   title = {SULTAN: Simultaneous User Learning and Task Execution, and its Application in Assistive Robotics},
   url = {http://www.iros2011.org/WorkshopsAndTutorialsProceedings/MW3/iros11ws_assistive_robotics.pdf},
   year = {2011},
}
@article{jardon2012experience,
   abstract = {This paper describes an innovative modeling and training framework and an simulator application for micro tunneling machines under heterogeneous gravel and sand soils. From the selective collection of skilled pilots' know-how of a pipe jacking microtunnelling machine in operation, to generate a rule-based system based on grouped rules and states that replicates machine performance. The adjustment of these states and associated rules allows creation, setup and analysis of a realistic functional model for tunneling machines. The system integrates a friendly human machine interface (HMI) that closely replicates real machine's pilot cabinet and allows natural interaction with the implemented inference engine through the simulated control panels. Additionally, the framework allows the training of tunneling machine's operators by simulation and subse- quent gathered data analysis. The virtual pilot's desk is the first implementation of a jack piping microtunnel- ing machine simulator by means of pilot's steering know-how capture methodology.},
   author = {Alberto Jardón and Juan G. Victores and Santiago Martínez and Carlos Balaguer},
   doi = {10.1016/j.autcon.2011.12.002},
   issue = {0},
   journal = {Automation in Construction},
   keywords = {Pipe jacking training,Robotic automation,Simulator,Tunnel execution modeling,Tunneling machine},
   pages = {33-46},
   title = {Experience acquisition simulator for operating microtuneling boring machines},
   volume = {23},
   url = {http://dx.doi.org/10.1016/j.autcon.2011.12.002},
   year = {2012},
}
@article{jardon2011personal,
   abstract = {Increasingly disabled and elderly people with mobility problems want to live autonomously in their home environment. They are motivated to use robotic aids to perform tasks by themselves, avoiding permanent nurse or family assistant supervision. They must find means to rehabilitate their abilities to perform daily life activities (DLAs), such as eating, shaving, or drinking. These means may be provided by robotic aids that incorporate possibilities and methods to accomplish common tasks, aiding the user in recovery of partial or complete autonomy. Results are highly conditioned by the system’s usability and potential. The developed portable assistive robot ASIBOT helps users perform most of these tasks in common living environments. Minimum adaptations are needed to provide the robot with mobility throughout the environment. The robot can autonomously climb from one surface to another, fixing itself to the best place to perform each task. When the robot is attached to its wheelchair, it can move along with it as a bundle. This paper presents the work performed with the ASIBOT in the area of rehabilitation robotics. First, a brief description of the ASIBOT system is given. A description of tests that have been performed with the robot and several impaired users is given. Insight into how these experiences have influenced our research efforts, especially, in home environments, is also included. A description of the test bed that has been developed to continue research on performing DLAs by the use of robotic aids, a kitchen environment, is given. Relevant conclusions are also included.},
   author = {Alberto Jardón and Juan G. Victores and Santiago Martínez and Antonio Giménez and Carlos Balaguer},
   doi = {10.1109/TSMCC.2011.2159201},
   issue = {4},
   journal = {IEEE Trans. on Systems, Man, and Cybernetics, Part C: Applications and Reviews},
   keywords = {Climbing robots,Clinical trials,Homecare,Inclusive technologies,Portable robots,Rehabilitation robotics},
   pages = {561-570},
   title = {Personal Autonomy Rehabilitation in Home Environments by a Portable Assistive Robot},
   volume = {42},
   url = {http://dx.doi.org/10.1109/TSMCC.2011.2159201},
   year = {2011},
}
@inproceedings{gonzalezfierro2010educational,
   abstract = {Off-the-shelf robotic kits are found in almost all retail stores. Most share common elements like motors, sensors, and a computer processor that permits the robots to be programmed to move autonomously in their environment. In addition, the wide scale availability and relatively low cost have made the robotic kits increasingly popular among students and teachers. Robotics competitions that are held annually in most countries are especially popular. The experience presented here is about the efforts of a group of students to teach each other in a peer-to-peer fashion and to manage several contest-related projects.},
   author = {Miguel González-Fierro and Alberto Jardón and Santiago Martinez and Martin F Stoelen and Juan G. Victores and Carlos Balaguer},
   city = {Darmstadt, Germany},
   journal = {Proceedings of Int. Conf. on SIMULATION, MODELING and PROGRAMMING for AUTONOMOUS ROBOTS (SIMPAR2010) Workshops},
   keywords = {approaches,arduino,ceabot,humanoid robots},
   pages = {649-658},
   title = {Educational initiatives related with the CEABOT contest},
   url = {http://asrob.uc3m.es/svn/ASROBrepo/Documentos/articulos/29-TeachingRobotics.pdf},
   year = {2010},
}
@inproceedings{stoelen2010information,
   abstract = {The development of intelligent service robotic systems is currently an active field of research in the robotics community. For example assistive robots that can aid elderly and disabled people in daily life activities. One emerging requirement for this type of system is the inclusion of the user in the decision process through physical and cognitive collaboration. This human-in-the-loop (HIL) concept allows for the use of the human perception and cognitive abilities in order to safely achieve the tasks that would be too complex to perform in a purely autonomous way. However, the overall human-machine system is complex and may be difficult to analyze. The user and the robot are operating in a closed loop and both are potentially capable of adapting to the other. The users may have a disparate set of noisy channels available for communicating their intended commands to the robot. The robots are typically dexterous and are expected to operate in an unstructured environment. Metrics can help in the analysis, development, and benchmarking of this type of system, by quantifying performance and driving the mutual learning and adaptation process. However, there are currently few such metrics available. Information Theory and related information-based concepts have been applied in disparate fields such as communications, human factors, control theory and cognitive processes. The work presented here attempts to identify metrics based on these concepts for assistive human- in-the-loop cognitive systems.},
   author = {Martin F Stoelen and Alberto Jardón and Juan G. Victores and Carlos Balaguer and Fabio Bonsignorio},
   issue = {c},
   journal = {Workshop on Good Experimental Methodology in Robotics and Replicable Robotics Research, Robotics Science and Systems (RSS)},
   pages = {1-6},
   publisher = {Zaragoza. Spain.},
   title = {Information Metrics for Assistive Human-In-The-Loop Cognitive Systems},
   url = {http://www.heronrobots.com/EuronGEMSig/Downloads/Zaragoza/rss2010_gemrrrr.pdf},
   year = {2010},
}
@inproceedings{jardon2009asibot,
   abstract = {New mobile robotic devices are conquering homes. From automatic shades to motorized vacuum cleaning units, advanced technologies are progressively being introduced into real domestic home environments. Technology is no longer being introduced to simply serve information or environmental control. Dynamic and mobile elements are being introduced to perform “household chores” that require dexterous manipulation and advanced sensing and reasoning. This is a huge objective that implies great improvement and advances in current robotic technologies related to anytime availability, safety, and user satisfaction. From the point of view of dependability, the most complex part of a house is the kitchen, attending to the number of static-fixed task devices (white appliances). This is the proposed working scenario, to test the acceptance of a new modular type of robotic aids for handicapped. The ASIBOT-based Domestic Aided Kitchen is the adaptation of a handicapped-adapted kitchen for the operation of the portable climbing robot ASIBOT. This paper presents the first results of simulation of ASIBOT and derived synthesized models in a dynamic VR model of the kitchen, and the current state of investigation for ASIBOT2 and its full integration with the kitchen and user.},
   author = {Alberto Jardón and Juan G. Victores and Martin F Stoelen and Santiago Martinez and Carlos Balaguer},
   city = {Corfu, Greece},
   doi = {10.1145/1579114.1579175},
   isbn = {9781605584096},
   journal = {Proceedings of the 2nd International Conference on PErvsive Technologies Related to Assistive Environments (PETRA)},
   keywords = {assistive robot,task oriented design,vr simulation},
   pages = {611-614},
   publisher = {ACM Press},
   title = {ASIBOT assistive robot in a domestic environment},
   url = {http://portal.acm.org/citation.cfm?doid=1579114.1579175},
   year = {2009},
}
@article{victores2011robot,
   abstract = {This article describes an unprecedented alternative to manual procedures for the application of advanced composite materials, such as Fiber Reinforced Polymer (FRP) and epoxy resins. A complete mobile integrated system is presented for the inspection and maintenance of concrete surfaces in tunnels. It allows performance of operations with minimum interference on passing traffic. The core of this system resides in a specially designed light-weight robotic tool, which is sensed and automated for processes. Sensing includes vision and a laser telemeter to assure precise inspection, superficial preparation, and composite application. The designed interconnection flange allows simple and robust attachment of the tool to a robotic arm's tip. The robottool set is to be mounted on a standard articulated lift platform. Therefore, an operator can direct the platform and the robottool set's operations from a control station placed at ground-level, in a wheeled vehicle on which the articulated lift platform is mounted. A graphical HumanMachine Interface (HMI) has been developed for the system. It allows the operator to identify fissures for the injection of epoxy resin, and weakened surfaces for FRP adhesion. Actual procedures are planned and performed by the system's automatic components.},
   author = {Juan G. Victores and Santiago Martinez and Alberto Jardón and Carlos Balaguer},
   doi = {10.1016/j.autcon.2010.12.005},
   issn = {09265805},
   issue = {5},
   journal = {Automation in Construction},
   keywords = {Concrete inspection,HMI,Robotic automation,Robotic tool design,Tunnel maintenance},
   pages = {629-636},
   publisher = {Elsevier B.V.},
   title = {Robot-aided tunnel inspection and maintenance system by vision and proximity sensor integration},
   volume = {20},
   url = {http://dx.doi.org/10.1016/j.autcon.2010.12.005},
   year = {2011},
}
@incollection{balaguer2010robotic,
   author = {Carlos Balaguer and Juan G. Victores},
   journal = {Technology Innovation in Underground Construction},
   pages = {445-460},
   publisher = {CRC Press},
   title = {Robotic tunnel inspection and repair},
   url = {http://www.crcpress.com/product/isbn/9780415551052},
   year = {2010},
}
@inproceedings{victores2009robot,
   abstract = {This paper describes an innovative alternative to manual procedures for the application of carbon fiber and resin injection in concrete surfaces in tunnels. It is based on a specially designed light-weight integrated tool for automatic application of Fiber Reinforced Polymer (FRP) and epoxy resin injection. Vision and laser telemeter sensing are integrated into the tool to assure precise inspection and maintenance operations. An interconnection flange allows simple and robust attachment to a robotic manipulator's tip. The robot-tool set is mounted on an articulated lift platform. Therefore, an operator can direct the platform and the robot-tool's operations in a control station placed at ground-level, in a wheeled vehicle on which the articulated lift platform is mounted. An intuitive Human-Machine Interface (HMI) has been developed to allow the operator to identify fissures for the injection of epoxy resin, and to choose where to place the FRP strips. Actual procedures are completely automatic.},
   author = {Juan G. Victores and Santiago Martinez and Alberto Jardón and Carlos Balaguer},
   city = {Austin, TX. USA.},
   journal = {26th International Symposium on Automation and Robotics in Construction (ISARC)},
   keywords = {concrete inspection,hmi,robotic automation,robotic tool design,tunnel maintenance},
   pages = {420-426},
   title = {Robot-aided Tunnel Inspection and Maintenance System},
   url = {http://www.iaarc.org/publications/fulltext/Robot-aided_Tunnel_Inspection_and_Maintenance_System.pdf},
   year = {2009},
}
